\section{Methodology}
\label{sec:methodology}

\subsection{The Manifold}
\label{subsec:methodology:manifold}

The Manifold is built from a dataset and a distance function.
% describe creation of Cluster Tree
We start by calculating a divisive-hierarchical clustering on the data.
This gives us a tree of Clusters, with the root containing every point in the dataset, and the leaves containing single points from the dataset.
The procedure is detailed in~\cite{ishaq2019entropy}.

% describe Graph and graph invariant.
Each Cluster has a center, i.e.\ the geometric median of points contained in that cluster, and a radius, i.e.\ the distance to the farthest point from the center.
Nearby clusters can sometimes have overlapping volumes, i.e.\ the distance between their centers is less than or equal to the sum of their radii.
We define a Graph as containing clusters as the nodes.
Overlapping clusters have an edge connecting them in the Graph.
Further, a Graph has the following two properties:
\begin{itemize}
    \item The clusters in the graph collectively contain every point in the dataset.
    \item Each point in the dataset is in exactly one cluster in the graph.
\end{itemize}
% Go over layer-graphs and optimal-graphs.
A Graph can be built from clusters at a fixed depth in the cluster-tree, or can contain clusters from multiple different depths in the tree.

% describe Manifold as containing the tree and all the graphs.
We use the Manifold to keep track of the cluster-tree and any associated Graphs.
The algorithms described in~\ref{subsec:methodology:individual-algorithms} work using one of these Graphs.

% describe individual algorithms.
\subsection{Individual Algorithms}
\label{subsec:methodology:individual-algorithms}


% describe meta-ml training-data creation.
% justify meta-ml model choice. describe integration of meta-ml model into manifold.
% describe selection criteria for optimal-graphs.
% describe ranking for selecting clusters for optimal-graphs.
% describe ensemble-model built from all optimal-graphs.
% describe extension to other meta-ml models.


%Given the manifold and induced graphs produced by CLAM (described in Section~\ref{subsec:related-works:chaoda}), we have implemented five distinct algorithms for detecting anomalies and outliers, all of which take advantage of either the hierarchical nature of the cluster-tree or the graph from a layer of clusters at a given depth.
%
%\subsection{Cluster Cardinality}
%\label{subsec:methodology:cluster-cardinality}
%
%For this method, we equate anomalousness with the cardinality of the cluster to which a point belongs.
%If a point belongs to a cluster with a very small number of points, it is more likely that the point is an anomaly or an outlier.
%Clusters that have a large cardinality are treated as more likely to contain normal points.
%
%\subsection{Parent-Child}
%\label{subsec:methodology:parent-child}
%
%In this approach, we examine the ratio of the cardinality of a cluster with that of its parent.
%For a given depth of the tree, a point is considered to be more anomalous if the cluster it belongs to is significantly smaller than its parent.
%This relationship tries to capture the intuition that those points which live farther away from the manifold should fall into increasingly smaller clusters as those clusters are partitioned.
%
%\subsection{Graph Neighborhood}
%\label{subsec:methodology:graph-neighborhood}
%
%For this approach, we rely on the graph induced by CLAM at a particular depth of the tree, as explained in Section~\ref{subsec:related-works:chaoda}.
%Given that graph and a point we consider the number of clusters reachable within a graph distance of $k$ from the point's cluster.
%The more clusters which are reachable from a given cluster, the \textit{less} likely it is to contain anomalous points.
%
%\subsection{Random Walk}
%\label{subsec:methodology:random-walk}
%
%Here, we implement the Outrank~\cite{moonesinghe2008outrank} algorithm with one key difference: since the graph induced by CLAM at a given depth of the tree may contain many disconnected components, some parts of the graph may be unreachable from any given cluster.
%The general idea behind this approach is to examine the frequency with which clusters are visited during $n$ random walks.
%Clusters that are visited frequently are less likely to be anomalous.
%Clusters that are visited infrequently are more likely to be on the periphery of the graph and distant from the underlying manifold.
%This approach assumes that $outrank(k)$ returns a hash table whose keys are the clusters of the graph, and whose values are the number of visits by a random walk to each cluster.
%
%\subsection{Subgraph Cardinality}
%\label{subsec:methodology:subgraph-cardinality}
%
%This approach is similar to Cluster Cardinality.
%Here, we utilize the cardinality of each connected component of the graph at various depths to determine anomalousness.
%As with some of our other approaches, we postulate that clusters which are members of small disconnected components are themselves more likely to contain anomalous points.
%
%\subsection{Normalization}
%\label{subsec:methodology:normalization}
%
%Due to the wide range of possible ``anomalousness'' scores from our methods, as a final step, we normalize all of our measurements by scaling them.
%For this work, we utilized Min-Max Scaling, shown in Equation~\ref{sec:methods:min-max-normalizationn}, to ensure that anomalousness values always in the $[0, 1]$ range.
%This lets us compare anomalousness of points fairly across our various methods.
%
%\begin{gather}
%x^{\prime} = \frac{x - x_{min}}{x_{max} - x_{min}}
%\label{sec:methods:min-max-normalizationn}
%\end{gather}
%
%\subsection{Fractal Dimension}
%\label{subsec:methodology:fractal-dimension}
%
%We compute the \textit{local fractal dimension} of a cluster using the same definition as in~\cite{ishaq2019entropy}.
%We define local fractal dimension as:
%
%\begin{gather}
%    \log_2\bigg(\frac{|B_D(q, r_1)|}{|B_D(q, r_2)|}\bigg)
%    \label{fractal-dimension}
%\end{gather}
%
%Where $B_D(q,r)$ is the set of points contained in a ball on the dataset $D$ of radius $r$ centered on a point $q$; here, fractal dimension is computed for a radius $r_1$ and a smaller radius $r_2=\frac{1}{2} \cdot r_1$.
%
%\subsection{Datasets}
%\label{subsec:datasets}
%
%For this work, we applied CHAODA to 24 different datasets, all sourced from Outlier Detection Datasets (ODDS)~\cite{rayana2016odds}.
%Datasets on ODDS are taken from the UCI Machine Learning Repository~\cite{UCIMLR}, and are standardized for Anomaly and Outlier detection benchmarks.
%
%The \textbf{lympho} dataset is derived from the Lymphography dataset.
%The two minority classes (with 2 and 4 points respectively) are combined to form the outliers.
%
%The \textbf{wbc} dataset is derived from the Wisconsin-Breast Cancer dataset.
%The malignant class is downsampled to 21 points, which are considered outliers.
%
%The \textbf{breastw} dataset is also derived from the Wisconsin-Breast Cancer dataset.
%The dimensionality is reduced to 9 and the points in the malignant class are considered outliers.
%
%The \textbf{glass} dataset is a glass identification dataset inspired by criminological investigations.
%Points in the minority class, class 6, are marked as outliers.
%
%The \textbf{vowels} dataset is derived from the Japanese Vowels dataset.
%For each utterance by speakers in the original data, 12 features are derived for anomaly detection.
%Utterances by speaker 1 are downsampled to 50 instances and marked as outliers.
%
%The \textbf{cardio} dataset is derived from the Cardiotocography dataset.
%Points in the normal class form inliers while those in the pathologic class, downsampled to 176 points, form the outliers.
%
%The \textbf{thyroid} dataset is derived from the thyroid disease dataset.
%The hyperfunction class, containing 93 instances, is treated as the outlier class.
%There are a total of 3,772 points in this dataset.
%
%The \textbf{annthyroid} dataset is also derived from the the thyroid disease dataset.
%The train and test sets are combined to give a total of 7,200 points.
%The hyperfunction class, containing 534 instances, is treated as the outlier class.
%
%The \textbf{musk} dataset is derived from its namesake in the UCI Machine Learning Repository.
%Three non-musk classes form inliers, two musk classes form outliers, and all other classes are discarded.
%
%The \textbf{satimage-2} dataset is derived from the Satlog dataset.
%Class 2 is downsampled to form 71 outliers.
%
%The \textbf{pima} dataset is derived from the Pima Indians diabetes dataset.
%Among other constraints, this dataset was preprocessed to include only female patients at least 21 years of age.
%
%The \textbf{satellite} dataset is derived from the Statlog dataset.
%The smallest three classes (2, 4, and 5) are considered outliers while the other classes are combined to form the inliers.
%
%The \textbf{shuttle} dataset is also derived from the Statlog dataset.
%Here, the smallest five classes (2, 3, 5, 6, and 7) are combined to form the outliers while class 1 forms the inliers.
%Class 4 is discarded.
%
%The \textbf{arrhythmia} dataset is derived from a 274-dimensional dataset.
%The five categorical attributes are discarded and the eight smallest classes (3, 4, 5, 7, 8, 9, 14, and 15) are combined to form the outliers.
%
%The \textbf{ionosphere} dataset consists of 33-dimensional points.
%The `bad' class is considered as the outlier class.
%
%The \textbf{mnist} dataset is derived from the classic MNIST dataset of handwritten digits.
%Digit-zero is considered the inlier class while 700 images sampled from digit-six are the outliers.
%Furthermore, 100 features are randomly selected from the original 784.
%
%The \textbf{optdigits} dataset is derived from the Optical Recognition of Handwritten Digits dataset.
%Digits 1--9 are the inliers while 150 samples of digit-zero form the outliers.
%
%The \textbf{http} dataset is derived from the original KDD Cup 1999 dataset.
%Its 41 attributes are reduced to 4, and the attacks are downsampled to form 3,377 outliers.
%This dataset contains a total of 567,479 points.
%
%The \textbf{smtp} is also derived from the KDD Cup 1999 dataset.
%There are 4 attributes to use for prediction.
%This version of the dataset only contains 95,156 points, of which 30 are outliers.
%
%The \textbf{cover} dataset is derived from the Forest Cover type dataset.
%The 10 quantitative attributes of each instance are used for outlier detection.
%Class 2 forms the normal points while class 4 forms the anomalies.
%The remaining classes are discarded.
%
%The \textbf{mammography} dataset is derived from the original Mammography dataset provided by Aleksandar Lazarevic.
%It contains 11,183 samples with 260 calcifications that form the outlier class.
%
%The \textbf{pendigits} dataset is derived from the Pen-Based Recognition of Handwritten Digits dataset from the UCI Machine Learning Repository.
%The original collection of handwritten samples is reduced to 6,870 points, of which 156 are outliers.
%
%The \textbf{wine} dataset is a collection of results of a chemical analysis of wines from a region in Italy.
%Classes 2 and 3 form the inliers while class 1, downsampled to 10 instances, is the outlier class.
%
%The \textbf{vertebral} dataset is a 6-dimensional dataset build by Dr. Henrique da Mota.
%The `Abnormal' class of 210 instances are used as inliers while the `Normal' class is downsampled to 30 instances to form the outliers.
%
%\subsection{Comparisons to Other Approaches}
%\label{subsec:methodology:comparisons-to-other-approaches}
%
%For each of the 24 datasets from ODDS, we applied each of the five CHAODA algorithms with each of three distance functions: Cosine distance, Euclidean (L2) distance, and Manhattan (L1) distance.
%After, allowing CLAM to continue partitioning the data until the underlying manifold had thoroughly shattered, we iterated over depth, applying each algorithm at every level and producing the corresponding ROC curve.
%In order to compare our methods, we used a standard one-class SVM from the Python \texttt{sklearn} package, and compared the area under a ROC curve (ROC AUC) to those reported by several state-of-the-art anomaly detection approaches: LOF~\cite{breunig2000lof}, HiCS~\cite{keller2012hics}, LODES~\cite{sathe2016lodes}, iForest~\cite{liu2008isolation}, Mass~\cite{ting2009mass}, MassE~\cite{aryal2014improving}, AOD (Active-Outlier)~\cite{abe2006outlier}, HST (HS-Tree)~\cite{tan2011fast}, and iNNE~\cite{bandaragoda2014efficient}.

\section{Methods}
\label{sec:methods}

\subsection{CLAM}
\label{subsec:methods:clam}

We present a manifold-mapping algorithm called CLAM (Clustered Learning of Approximate Manifolds).
% This is an extension of earlier work presented in~\cite{ishaq2019clustered}.

To start, we need a dataset and a distance function on the points in that dataset.
A dataset is a collection of $n$-points in a $D$-dimensional embedding space, $\textbf{X} = \{x_1 \dots x_n\}, x_i \in \mathbb{R}^D$.
A Distance Function takes two points in the dataset and deterministically produces a non-negative real number, $f : (\mathbb{R}^D, \mathbb{R}^D) \mapsto \mathbb{R}^+$

We require any distance function to have the following properties:
\begin{align*}
    \forall x \in X,    & \ f(x, x) = 0       \\
    \forall x, y \in X, & \ f(x, y) = f(y, x)
\end{align*}
The distance function may or may not obey the triangle-inequality, but in this paper we used $L1$ and $L2$ norms, which are metrics.

\subsubsection{Clustering}\label{subsubsec:methods:clam:clustering}
% describe creation of Cluster Tree
We start by building a divisive-hierarchical clustering of the data based on a random sampling of $\sqrt(k)$ datapoints, for $k$ points at a given level of the tree, which achieves clustering in expected $\mathcal{O}(n \lg n)$ time.
This gives us a tree of clusters, with the root containing every point in the dataset, and each leaf containing a single point from the dataset.
The CLAM clustering algorithm is defined in Algorithm~\ref{alg:clam}.
% The procedure is detailed in~\cite{ishaq2019clustered}.
% We can't cite our work as "us" during the review process

\begin{algorithm} % enter the algorithm environment
\caption{Cluster} % give the algorithm a caption
\label{alg:clam} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
\STATE $minsize \leftarrow 1$
\STATE $data$, a dataset.
\STATE $d \gets 0$
\STATE $clusters = \{\}$
\STATE $n = |data|$
\WHILE{true}
    \STATE $seeds \leftarrow \sqrt{n}$ random points from $data$
    \STATE $l, r \gets \{l, r | l,r \in seeds \land l,r = \argmax d(x,y) | x,y \in seeds\}$
    \STATE $clusters[l] \gets \{x | x \in data \land d(l,x) \le d(r,x)\}$
    \STATE $clusters[r] \gets \{x | x \in data \land d(r,x) < d(l,x)\}$
    \IF{$|clusters[l]| > minsize$}
        \STATE Cluster(clusters[l])
    \ENDIF
    \IF{$|clusters[r]| > minsize$}
        \STATE Cluster(clusters[r])
    \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Several properties of clusters are interesting and important to consider, including \textit{cardinality}, the number of points in a cluster; \textit{center}, the geometric median of points contained in a cluster; \textit{radius}, the distance to the farthest point from a center; and \textit{local fractal dimension}, as described in~\cite{ishaq2019clustered}.
We can also consider individual \textit{parent-child ratios} of cardinality, radius, and local fractal dimension, as well as \textit{exponential moving averages} of the parent-child ratios along a branch of the tree.
Each of these properties is computed during the process of clustering, and stored as metadata for each cluster.
In particular, we use the parent-child ratios and the exponential moving averages of those ratios to help generalize our anomaly detection method from a small set of datasets to a large, distinct set of datasets.

\subsubsection{Graphs}
% TODO: describe Graph and graph invariant.
Clusters that are near each other in the embedding space sometimes have overlapping volumes, i.e.\ the distance between their centers is less than or equal to the sum of their radii.
We define a graph $G=(V,E)$ with the selected clusters in one-to-one correspondence to vertices, and with an edge between two vertices if and only if their corresponding clusters overlap.
Here, we define two clusters as overlapping if the distance between the cluster centers is less than or equal to the sum of the two clusters' radii.
Note that any given datapoint will only be a member of one cluster at a given level of the tree, even though it might exist inside another cluster's radius.
Thus, the clusters are not necessarily hyperspheres, but rather polytopes akin to a high-dimensional version of a Voronoi diagram~\cite{voronoi1908nouvelles}.
For our purposes, a graph also exhibits a particular invariant.
The clusters corresponding to vertices in the graph collectively contain every point in the dataset.
In addition, each point in the dataset is in exactly one cluster corresponding to exactly one vertex in the graph.

A corollary to this invariant is that the graph will never contain two vertices such that one vertex corresponds to a parent or child cluster of another vertex.
% Go over layer-graphs and optimal-graphs.
A Graph can be built from clusters at a fixed depth in the cluster-tree (a layer-graph), or from clusters from multiple different depths in the tree (an optimal-graph).
In this work we consider the cardinality of a graph to be \textit{vertex cardinality}, i.e.\ the number of vertices (clusters) in the graph.

\subsubsection{The Manifold}
% describe Manifold as containing the tree and all the graphs.
According to the ``manifold hypothesis''~\cite{fefferman2016testing},
datasets that come from constrained generating processes and are embedded in a high-dimensional space typically only occupy a low-dimensional manifold in that embedding space.

The graphs discussed so far map this low dimensional manifold in the original embedding space.
Different graphs do this at different levels of local and/or global resolution.
Our aim is to properly build such a graph, where different levels of resolution may be necessary for different regions of the manifold.
We can then apply several anomaly detection algorithms to these graphs.
These algorithms will often also incorporate information from the tree, such as parent-child cardinality ratios.

We describe some algorithms in Section~\ref{subsec:methods:individual-algorithms}.
While these algorithms are themselves fairly simple, the real challenge is in selecting the right clusters for the graphs for the algorithms operate on.
We will demonstrate CLAM's manifold mapping to be effective enough that even these simple algorithms, more often than not, outperform state-of-the-art anomaly detection algorithms.

\subsection{Induced Graphs}
\label{subsec:methods:induced-graphs}
The heart of the problem with our methods of anomaly detection is building the right graph to represent the underlying manifold.
One could try using every possible combination of clusters to form a graph but this quickly leads to combinatorial explosion.
Instead, we must intelligently select those clusters that, when used to build the graph, perform best for anomaly detection.

Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) is often used to measure the performance of anomaly detectors;
we wish to choose clusters that are expected to maximize this measure.
We can accomplish this by learning a function that takes a cluster and predicts its contribution to AUC if that cluster were selected for the graph.
Any function of the following form will suffice: $f: Cluster \mapsto \mathbb{R}^+$.


We chose a simple linear regression to fill this role.
Such a model needs some data to train with.
To generate this data, we took a random sample of some of the datasets described in Section~\ref{subsec:methods:datasets}, choosing six datasets at random which had between 1000 and 100,000 samples.
We generate CLAM manifolds for these training datasets, use the linear regression model to learn from these datasets, and apply the results to an entirely different collection of datasets. The process is as follows.

We generate the initial training data by considering layer-graphs.
For each such graph, we calculate the means of the cardinality ratio, radius ratio, and local fractal dimension ratio;
along with the exponential moving averages of these ratios, described in Section~\ref{subsubsec:methods:clam:clustering}.
These ratios form the feature vector for one training sample.
For each method described in Section~\ref{subsec:methods:individual-algorithms}, we apply it to the graph and obtain an AUC ROC\@.
We then train the regression model to predict this AUC from the features extracted from the graph.
The result is a separate regression model for each algorithm of CHAODA, trained from the six training datasets; the weights learned here are then used for all test datasets, regardless of their dimensionality or other properties.


% describe individual algorithms.
\subsection{Individual Algorithms}\label{subsec:methods:individual-algorithms}

Here we describe several simple methods for anomaly detection.
Each of these methods uses a graph of clusters from CLAM to calculate an anomalousness score for each point in the dataset.

For each algorithm, $scores$ is a dictionary of clusters and their outlier scores,
$V$ is the set of clusters in a graph,
$E$ is the set of edges in a graph, and
the cardinality of a cluster $c$ is denoted by $|c|$, represneting the number of points in a cluster.

\subsubsection{Relative Cluster Cardinality}
We measure the anomalousness of a point by the cardinality of the cluster that point belongs to relative to the cardinalities of the other clusters in the graph.
Points in the same cluster are considered equally anomalous and points in clusters with relatively low cardinalities are considered more anomalous than points in clusters with relatively high cardinalities. The algorithm is defined in Algorithm~\ref{alg:rclc}. The time complexity of this algorithm is $O(|V|)$.


% \begin{itemize}
%     \item for each cluster $c$ in graph $g$:
%     \begin{itemize}
%         \item $scores[c] \leftarrow -|c|$.
%     \end{itemize}
% \end{itemize}

\begin{algorithm}[h]
    \caption{Relative Cluster Cardinality}
    \label{alg:rclc}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {cluster $c \in G$}
    \STATE $scores[c] \gets -|c|$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Relative Component Cardinality}
We define components of a graph by the property that no two clusters from different components have an edge between them, and every pair of clusters in the same component has a path between them.
Consider the relative cardinalities of each component in much the same we we considered the relative cardinalities of clusters in the Relative Cluster Cardinality method.
Points in clusters in relatively small components are considered more anomalous than points in clusters in relatively large components, and points in clusters in the same component are considered equally anomalous. The algorithm is defined in Algorithm~\ref{alg:rcc}. This algorithm first finds the components of the graph, so its time complexity is $O(|E| + |V|)$.

\begin{algorithm}[h]
    \caption{Relative Component Cardinality}
    \label{alg:rcc}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {component $C \in G$}
        \STATE $scores[C] \gets -|C|$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Graph Neighborhood Size:}
Given the graph with clusters and edges, consider the number of clusters reachable from a starting cluster within a given graph distance $k$.
We call this number the \textit{graph-neighborhood} of the starting cluster.
With $k$ relatively small compared to the diameter of the graph, we can consider the relative sizes of \textit{graph-neighborhood} of the clusters in the graph.
Points in clusters with small graph-neighborhoods are considered more anomalous than points in clusters with large graph-neighborhoods. The algorithm is defined in Algorithm~\ref{alg:gns}.
This algorithm is dominated by the computation of the eccentricity of each cluster.
Its time complexity is $O(|E| \cdot |V|)$.


\begin{algorithm}[h]
    \caption{Graph Neighborhood Size}
    \label{alg:gns}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \REQUIRE $f \in \mathbb{R}$ in the range $(0,1]$ ($0.25$ by default).
    \FOR {cluster $c \in G$}
        \STATE $e_c \gets$ the eccentricity of $c$
        \STATE $s \gets e_c \times f$
        \STATE perform a breadth-first traversal from $c$ with $s$ steps
        \STATE $v \gets$ the number of unique clusters visited by the traversal
        \STATE $scores[c] \gets -v$
    \ENDFOR
\end{algorithmic}
\end{algorithm}



\subsubsection{Child-Parent Cardinality Ratio}
% As described in CHESS~\cite{ishaq2019clustered},
As described in Section~\ref{subsubsec:methods:clam:clustering}, a cluster is partitioned by using its two maximally distant points as poles.
The points are split among children by whichever pole they are closer to.
Consider the fraction of points in a cluster that are assigned to each child.
If a child cluster only contains a small fraction of the points that its parent did, then we consider the points in that child cluster to be more anomalous.
These child-parent cardinality ratios are accumulated for each point down its branch in the tree, terminating when the child cluster is a node in the graph.
Points with a low value of these accumulated ratios are considered more anomalous than points with a high value of these accumulated ratios. The algorithm is defined in Algorithm~\ref{alg:cpcr}. The time complexity of this algorithm is $O(|V|)$.

\begin{algorithm}[h]
    \caption{Child-Parent Cardinality Ratio}
    \label{alg:cpcr}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {cluster $c \in G$}
        \STATE $p \gets$ the parent cluster of $c$
        \STATE $scores[c] \gets \frac{|p|}{|c|}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Random Walks}
We perform long random walks on each component of a graph, and we count the number of times each cluster was visited.
We consider the relative visitation counts among the clusters to be inversely related to their anomalousness.
The least visited clusters are anomalous, and the most visited clusters are not. The algorithm is defined in Algorithm~\ref{alg:rw}.
This algorithm is dominated by the computation of the transition matrix for the graph.
Its worst-case time complexity is $O(|V|^2)$, but is often better because the transition matrix is computed separately for each component in the graph.

The algorithm is as follows:

\begin{algorithm}[h]
    \caption{Random Walks}
    \label{alg:rw}
\begin{algorithmic}[1]
    \REQUIRE $G = (V,E)$, a graph
    \STATE Initialize $v[c] \gets 0 \forall c \in G$
    \STATE Initialize $s$ to be $10 \times |V|$
    \FOR {cluster $c \in G$}
        \STATE $c' \gets c$
        \FOR{$i=1$ {\bfseries to} $s$}
            \STATE $c' \gets n(d),$ a random neighbor of $d$
            \STATE increment $v[c']$
        \ENDFOR
    \ENDFOR
    \FOR {cluster $c \in G$}
        \STATE $scores[c] \gets \frac{|p|}{|c|}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Stationary Probabilities}
We compute the transition probability matrix of each component of a graph that contains at least two clusters.
We then compute successive powers of this matrix. % TODO citation
This process will eventually converge (add citation here), and we find this convergent matrix.
The sum of the values along a row is inversely related to the anomalousness of the respective cluster. The algorithm is defined in Algorithm~\ref{alg:sp}.
This algorithm is dominated by the computation of the transition matrix for each component in the graph.
Its time worst-case time complexity is $O(|V|^2)$, but is often better in practice.

\begin{algorithm}[h]
    \caption{Stationary Probabilities}
    \label{alg:sp}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {component $C \in G$}
        \STATE $M \gets$ the transition matrix for $C$
        \REPEAT
            \STATE $M \gets M^2$
        \UNTIL $M$ converges
        \FOR {cluster $c \in C$}
            \STATE $s \gets $ the row from $M$ corresponding to $c$
            \STATE $scores[c] \gets -\Sigma(s)$ 
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}




\subsection{Normalization}\label{subsec:methods:normalization}
The individual methods produce outlier scores for each cluster, rather than for each point.
We remedy this by having each point simply inherit the outlier score of the cluster it belongs to.
Since our graphs guarantee that each point is in exactly one cluster, and that every point from the dataset is accounted for, this inheritance assigns an outlier score to each point in a dataset.

However, the individual methods still produce outlier scores in a wide range of values.
The only common feature among these scores is that low scores correspond to inliers and high scores correspond to outliers.
Thesefore, scores from differnt methods or graphs cannot be directly compared to each other.
We need some method for normalizing these scores to the same range.
The standard range is $[0, 1]$, with high scores assigned to outliers.

We use gaussian normalization by default, though we include sigmoid normalization and min-max normalization as options in our implementation. (add possible citation for paper on normalizing such scores)

% TODO: Add equations and/or citations for normalization methods.

Once the scores are in a uniform range, we are ready for the ensemble.


\subsection{Ensemble}\label{subsec:methods:ensemble}
Given the set of regression models learned from the training datasets described in Section~\ref{subsec:methods:induced-graphs}, we can use it to build graphs for any other dataset, even if those datasets differ significantly in dimensionality and size; essentially, this is a form of transfer learning.
We use the cluster ratios and the associated regression constants to rank every cluster in a CLAM tree.
These rankings are normalized by the cardinality of each cluster.
The highest ranked clusters are then used to build a graph.
This graph is then used with the corresponding individual algorithm to calculate anomaly scores for all points in the dataset.
The scores from each individual algorithm are then combined into an ensemble.
We present the AUC scores from this ensemble in Section~\ref{sec:results}.

% TODO fully explain ensemble methods
% TODO: Add a note on speed and the settings between CHAODA-fast and CHAODA


\subsection{Datasets}\label{subsec:methods:datasets}

We sourced 24 datasets, all from Outlier Detection Datasets (ODDS)~\cite{rayana2016odds}, for training CHAODA and testing its performance.
All of these datasets are adapted from the UCI Machine Learning Repository (UCIMLR)~\cite{UCIMLR}, and have been standardized, by ODDS, for anomaly and outlier detection benchmarks.



% TODO: Did we repeat this experiment with a different six? Was it actually at random? Najib: I'm planning on upping this to 8 datasets, and then doing 4-fold cross-validation. This selection was completely random. np.random.seed(42)
Of these 24 datasets, we selected a random sample of 6 datasets, shown in Table~\ref{table:results:train-performance}, to use for training CHAODA\@.
The remaining datasets were used to measure the performance of CHAODA\@.
The test set of datasets is split among Tables~\ref{table:results:test-performance-1} and~\ref{table:results:test-performance-2}.

We provide more details on the datasets, including their size, dimensionality, and fraction of outliers, in the supplementary materials.

Note that CHAODA is an unsupervised algorithm for outlier detection.
As such, we compare only against other unsupervised algorithms.
We compared against $n$ unsupervised algorithms implemented in pyOD~\cite{zhao2019pyod}, as well as (list others here). % TODO fill this in
A supervised version of CHAODA will be possible future work.


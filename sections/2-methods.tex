\section{Methods}
\label{sec:methods}

\subsection{CLAM}
\label{subsec:methods:clam}

We present a Manifold-Mapping algorithm called CLAM (Clustered Learning of Approximate Manifolds).
This is an extension of earlier work presented in~\cite{ishaq2019clustered}.

To start, we need a dataset and a distance function on the points in that dataset.
A dataset is a collection of $n$-points in a $D$-dimensional embedding space.
\begin{gather*}
    \textbf{X} = \{x_1 \dots x_n\}, x_i \in \mathbb{R}^D
\end{gather*}
A Distance Function takes two points in the dataset and deterministically produces a non-negative real number.
\begin{gather*}
    f : (\mathbb{R}^D, \mathbb{R}^D) \mapsto \mathbb{R}^+
\end{gather*}
We require any distance function to have the following properties:
\begin{align*}
    \forall x \in X,    & \ f(x, x) = 0       \\
    \forall x, y \in X, & \ f(x, y) = f(y, x)
\end{align*}
The distance function may or may not obey the triangle-inequality.

\subsubsection{Clustering:}\label{subsubsec:methods:clam:clustering}
% describe creation of Cluster Tree
We start by building a divisive-hierarchical clustering of the data based on a random sampling of $\sqrt(k)$ datapoints, for $k$ points at a given level of the tree, which achieves clustering in expected $\mathcal{O}(n \lg n)$ time.
This gives us a tree of clusters, with the root containing every point in the dataset, and each leaf containing a single point from the dataset.
The procedure is detailed in~\cite{ishaq2019clustered}.

Several properties of clusters are interesting and important to consider, including \textit{cardinality}, the number of points in a cluster; \textit{center}, the geometric median of points contained in a cluster; \textit{radius}, the distance to the farthest point from a center; and \textit{local fractal dimension}, as described in~\cite{ishaq2019clustered}.
We can also consider individual \textit{parent-child ratios} of cardinality, radius, and local fractal dimension, as well as \textit{exponential moving averages} of the parent-child ratios along a branch of the tree.
In particular, we use the parent-child ratios and the exponential moving averages of those ratios to help generalize our anomaly detection method from a small set of datasets to a large, distinct set of datasets.
/
\subsubsection{Graphs:}
% TODO: describe Graph and graph invariant.
Clusters that are near each other in the embedding space sometimes have overlapping volumes, i.e.\ the distance between their centers is less than or equal to the sum of their radii.
We define a graph $G=(V,E)$ with the selected clusters in one-to-one correspondence to vertices, and with an edge between two vertices if and only if their corresponding clusters overlap.
For our purposes, a graph also exhibits a particular invariant:
\begin{itemize}
    \item The clusters corresponding to vertices in the graph collectively contain every point in the dataset.
    \item Each point in the dataset is in exactly one cluster corresponding to exactly one vertex in the graph.
\end{itemize}
A corollary to this invariant is that the graph will never contain two vertices such that one vertex corresponds to a parent or child cluster of another vertex.
% Go over layer-graphs and optimal-graphs.
A Graph can be built from clusters at a fixed depth in the cluster-tree (a layer-graph), or from clusters from multiple different depths in the tree (an optimal-graph).
In this work we consider the cardinality of a graph to be \textit{vertex cardinality}, i.e.\ the number of vertices (clusters) in the graph.

\subsubsection{The Manifold:}
% describe Manifold as containing the tree and all the graphs.
According to the Manifold Hypothesis~\cite{fefferman2016testing},
datasets that come from constrained generating processes and are embedded in a high-dimensional space actually only occupy a low-dimensional manifold in that embedding space.

The graphs discussed so far map this low dimensional manifold in the original embedding space.
Different graphs do this at different levels of local and/or global resolution.
Our aim is to properly build such a graph, where different levels of resolution may be necessary for different regions of the manifold.
We can then apply several anomaly detection algorithms to these graphs.
These algorithms will often also incorporate information from the tree, such as parent-child cardinality ratios.

We describe some algorithms in Section~\ref{subsec:methods:individual-algorithms}.
While these algorithms are themselves fairly simple, the real challenge is in selecting the right clusters for the graphs for the algorithms operate on.
We will demonstrate CLAM to be such a powerful technique in Manifold-Mapping that even these simple algorithms, more often than not, outperform state-of-the-art anomaly detection algorithms.

\subsection{Induced Graphs:}
\label{subsec:methods:induced-graphs}
The heart of the problem with our methods of anomaly detection is building the right graph to represent the underlying manifold.
One could try using every possible combination of clusters to form a graph but this quickly leads to combinatorial explosion.
Instead, we must intelligently select those clusters that, when used to build the graph, perform best for anomaly detection.

Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) is often used to measure the performance of anomaly detectors;
we wish to choose clusters that are expected to maximize this measure.
We can accomplish this by learning a function that takes a cluster and predicts its contribution to AUC if that cluster were selected for the graph.
Any function of the following form will suffice.
\begin{gather*}
    f: Cluster \mapsto \mathbb{R}^+
\end{gather*}

We chose a simple linear regression to fill this role.
Such a model needs some data to train with.
To generate this data, we took a random sample of some of the datasets described in Section~\ref{subsec:methods:datasets}, choosing six datasets at random which had at between 1000 and 100,000 samples.
We generate CLAM Manifolds for these training datasets, use the linear regression model to learn from these datasets, and apply the results to an entirely different collection of datasets. The process is as follows.

We generate the initial training data by considering layer-graphs.
For each such graph, we calculate the means of the cardinality ratio, radius ratio, and local fractal dimension ratio;
along with the exponential moving averages of these ratios, described in Section~\ref{subsubsec:methods:clam:clustering}.
These ratios form the feature vector for one training sample.
For each method described in Section~\ref{subsec:methods:individual-algorithms}, we apply it to the graph and obtain an AUC ROC\@.
We then train the regression model to predict this AUC from the features extracted from the graph.
The result is a separate regression model for each algorithm of CHAODA.


% describe individual algorithms.
\subsection{Individual Algorithms}\label{subsec:methods:individual-algorithms}

Here we describe several simple methods for anomaly detection.
Each of these methods uses a graph of clusters from CLAM to calculate an anomalousness score for each point in the dataset.

For each algorithm:
\begin{itemize}
    \item $scores$ is a dictionary of clusters and their outlier scores,
    \item $V$ is the set of clusters in a graph,
    \item $E$ is the set of edges in a graph, And
    \item the cardinality of a cluster $c$ is denoted by $|c|$, represneting the number of points in a cluster.
\end{itemize}

\subsubsection{Relative Cluster Cardinality:}
We measure the anomalousness of a point by the cardinality of the cluster that point belongs to relative to the cardinalities of the other clusters in the graph.
Points in the same cluster are considered equally anomalous and points in clusters with relatively low cardinalities are considered more anomalous than points in clusters with relatively high cardinalities.

The algorithm is as follows:

\begin{itemize}
    \item for each cluster $c$ in graph $g$:
    \begin{itemize}
        \item $scores[c] \leftarrow -|c|$.
    \end{itemize}
\end{itemize}

The time complexity of this algorithm is $O(|V|)$.

\subsubsection{Relative Component Cardianlity}
We define components of a graph by the property that no two clusters from different components have an edge between them, and every pair of clusters in the same component has a path between them.
Consider the relative cardinalities of each component in much the same we we considered the relative cardinalities of clusters in the Relative Cluster Cardinality method.
Points in clusters in relatively small components are considered more anomalous than points in clusters in relatively large components, and points in clusters in the same component are considered equally anomalous.

The algorithm is as follows:

\begin{itemize}
    \item for each component $C$ in graph $g$:
    \begin{itemize}
        \item for each cluster $c$ in $C$:
        \begin{itemize}
            \item $scores[c] \leftarrow -|C|$.
        \end{itemize}
    \end{itemize}
\end{itemize}

This algorithm first finds the components of the graph, so its time complexity is $O(|E| + |V|)$.

\subsubsection{Graph Neighborhood Size:}
Given the graph with clusters and edges, consider the number of clusters reachable from a starting cluster within a given graph distance $k$.
We call this number the \textit{graph-neighborhood} of the starting cluster.
With $k$ relatively small compared to the diameter of the graph, we can consider the relative sizes of \textit{graph-neighborhood} of the clusters in the graph.
Points in clusters with small graph-neighborhoods are considered more anomalous than points in clusters with large graph-neighborhoods.

The algorithm is as follows:

\begin{itemize}
    \item let $f$ be a real number in the $(0, 1]$ range ($0.25$) works well.
    \item for each cluster $c$ in graph $g$:
    \begin{itemize}
        \item let $e_c$ be the eccentricity of $c$.
        \item let $s$ be $e_c * f$.
        \item perform a breadth-first traversal from $c$ with $s$ steps.
        \item let $size$ be the number of unique clusters visited by the breadth-first traversal.
        \item $scores[c] \leftarrow -size$.
    \end{itemize}
\end{itemize}

This algorithm is dominated by the computation of the eccentricity of each cluster.
Its time complexity is $O(|E| \cdot |V|)$.

\subsubsection{Child-Parent Cardinality Ratio:}
As described in CHESS~\cite{ishaq2019clustered}, a cluster is partitioned by using its two maximally distant points as poles.
The points are split among children by whichever pole they are closer to.
Consider the fraction of points in a cluster that are assigned to each child.
If a child cluster only contains a small fraction of the points that its parent did, then we consider the points in that child cluster to be anomalous.
These child-parent cardinality ratios are accumulated for each point down its branch in the tree, terminating when the child cluster is a node in the graph.
Points with a low value of these accumulated ratios are considered more anomalous than points with a high value of these accumulated ratios.

The algorithm is as follows:

\begin{itemize}
    \item for each cluster $c$ in graph $g$:
    \begin{itemize}
        \item let $p$ be the parent cluster of $c$.
        \item $scores[c] \leftarrow \frac{|p|}{|c|}$.
    \end{itemize}
\end{itemize}

The time complexity of this algorithm is $O(|V|)$.


\subsubsection{Random Walks:}
We perform long random-walks on each component of a graph, and we count the number of times each cluster was visited.
We consider the relative visitation counts among the clusters to be inversely related to their anomalousness.
The least visited clusters are anomalous, and the most visited clusters are not.

The algorithm is as follows:

\begin{itemize}
    \item let $visits$ be a dictionary of clusters and their visitation counts, initialized to $0$.
    \item let $s$ be a large number of steps for the random walk.
    \item for each cluster $c$ in graph $g$:
    \begin{itemize}
        \item perform a random walk from $c$ with $s$ steps and increment the value of the visited cluster in $visits$ with each step.
    \end{itemize}
    \item for each cluster $c$ in graph $g$:
    \begin{itemize}
        \item $scores[c] \leftarrow -visits[c]$.
    \end{itemize}
\end{itemize}

This algorithm is dominated by the computation of the transition matrix for the graph.
Its worst-case time complexity is $O(|V|^2)$, but is often better because the transition matrix is computed separately for each component in the graph.

\subsubsection{Stationary Probabilities:}
We compute the transition probability matrix of each component of a graph that contains at least two clusters.
We then compute successive powers of this matrix.
This process will eventually converge (add citation here), and we find this convergent matrix.
The sum of the values along a row is inversely related to the anomalousness of the respective cluster.

The algorithm is as follows:

\begin{itemize}
    \item for each component $C$ in graph $g$:
    \begin{itemize}
        \item let $M$ be the transition matrix for $C$.
        \item keep squaring $M$ until the matrix converges.
        \item let $S$ be this converged matrix.
        \item for each cluster $c$ in $C$:
        \begin{itemize}
            \item let $s$ be the respective row from $S$.
            \item $scores[c] \leftarrow -sum(s)$
        \end{itemize}
    \end{itemize}
\end{itemize}

This algorithm is dominated by the computation of the transition matrix for each component in the graph.
Its time worst-case time complexity is $O(|V|^2)$, but is often better in practice.


\subsection{Normalization}\label{subsec:methods:normalization}
The individual methods produce outlier scores for each cluster, rather than for each point.
We remedy this by having each point simply inherit the outlier score of the cluster it belongs to.
Since our graphs guarantee that each point is in exactly one cluster, and that every point from the dataset is accounted for, this inheritance assigns an outlier score to each point in a dataset.

However, the individual methods still produce outlier scores in a wide range of values.
The only common feature among these scores is that low scores correspond to inliers and high scores correspond to outliers.
Thesefore, scores from differnt methods or graphs cannot be directly compared to each other.
We need some method for normalizing these scores to the same range.
The standard range is $[0, 1]$, with high scores assigned to outliers.

We use gaussian normalization by default, though we include sigmoid normalization and min-max normalization as options in our implementation. (add possible citation for paper on normalizing such scores)

% TODO: Add equations and/or citations for normalization methods.

Once the scores are in a uniform range, we are ready for the ensemble.


\subsection{Ensemble}\label{subsec:methods:ensemble}
Given the set of regression models learned from the training datasets described in Section~\ref{subsec:methods:induced-graphs}, we can use it to build graphs for any other dataset, even if those datasets differ significantly in dimensionality and size; essentially, this is a form of transfer learning.
We use the cluster ratios and the associated regression constants to rank every cluster in a CLAM tree.
These rankings are normalized by the cardinality of each cluster.
The highest ranked clusters are then used to build a graph.
This graph is then used with the corresponding individual algorithm to calculate anomaly scores for all points in the dataset.
The scores from each individual algorithm are then combined into an ensemble.
We present the AUC scores from this ensemble in Section~\ref{sec:results}.

% TODO: Add a note on speed and the settings between CHAODA-fast and CHAODA


\subsection{Datasets}\label{subsec:methods:datasets}

We sourced 24 datasets, all from Outlier Detection Datasets (ODDS)~\cite{rayana2016odds}, for training CHAODA and testing its performance.
All of these datasets are adapted from the UCI Machine Learning Repository (UCIMLR)~\cite{UCIMLR}, and have been standardized, by ODDS, for anomaly and outlier detection benchmarks.

We provide a summary of the datasets in the supplementary materials.
For more details on each dataset, please refer to the supplementary materials.




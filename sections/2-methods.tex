\section{Methods}
\label{sec:methods}

% Overview
Here we provide an overview of the rest of this manuscript.
We start with a dataset and a distance metric (refer subsection).
CLAM applies hierarchical clustering to build a tree for this dataset (refer subsection).
We select clusters (refer subsection) from the tree and induce graphs (refer subsection).
We analyze the graphs, sometimes also using information from the tree, to capture information and properties that indicate anomalousness of points (refer subsection).
This lets us generalize from a training-set of datasets to an entirely disjoint testing-set of datasets (refer results).
We build meta-ml models (linear regression and decision-tree regressors) to use the individual algorithms and rank clusters by their anomaly-scores (refer subsection).
We train the meta-ml models in epochs, inducing new graphs for each new epoch from the models using the models trained on the previous epoch (refer subsection).
We use the set of trained meta-ml models to induce some number of final graphs that are used in an ensemble (refer subsection).

Note that during training, CHAODA is somewhat supervised.
CHAODA uses this training to learn a set of meta-ml models for selecting clusters and inducing graphs.
During testing, and inference, on new datasets, CHAODA is unsupervised and uses the learned meta-ml models.


\subsection{Dataset and Distance Function}
\label{subsec:methods:dataset-and-distance-function}

We start with a dataset $\textbf{X} = \{x_1 \dots x_n\}$ with $n$ points and a distance function $f : (\textbf{X}, \textbf{X}) \mapsto \mathbb{R}^+$.
The distance function takes two points from the dataset and deterministically produces a non-negative real number.
We also require that the distance function is symmetric and that the distance between two identical points is zero, i.e.\ $\forall x, y \in X$, $f(x, y) = f(y, x)$ and $f(x, y) = 0 \Leftrightarrow x = y$.
CLAM and CHAODA are general over any distance function that obeys these constraints.

CLAM assumes the ``manifold hypothesis''~\cite{fefferman2016testing}, i.e.\ datasets collected from constrained generating phenomena that are embedded in a high-dimensional space typically only occupy a low-dimensional manifold in that space.
CLAM and CHAODA learn the geometric and topological properties of these manifolds in a way that generalizes across datasets and distance function regardless of dataset specific properties such as total number of points, dimensionality, absolute distance values, etc.
We demonstrate this genericity by our anomaly detection performance in Section~\ref{sec:results}.

Note that we often speak of each datum as embedded in a $D$-dimensional Banach space and we use Euclidean notions, such as voids and volumes, to talk about the geometric and topological properties of the manifold.
However, these notions are only helpful for helping build intuition and as an aid to understanding.
Mathematically, CLAM does not rely on such notions; in-fact, the details of an embedding space can be abstracted away behind the distance function.

Also note that we can provide certain guarantees, see CHESS~\cite{ishaq2019clustered}, when the distance function used is a metric, i.e.\ it obeys the triangle inequality.
While CLAM and CHAODA work well with distance functions that are not metrics, we have not investigated how the lack of the triangle inequality changes, or breaks, those guarantees in the context of anomaly detection.
For this paper, we show results using the $L1$-norm and $L2$-norm.


\subsection{Clustering}
\label{subsec:methods:clustering}

We start by building a divisive hierarchical clustering of the dataset.
We partition, as described in~\ref{alg:partition}, a cluster with $k$ points using a pair of well-separated points from among a random sampling of $\sqrt k$ points.
Starting from a root-cluster containing the entire dataset, we continue until each leaf contains only one datum.
This achieves clustering in expected $\mathcal{O}(n \lg n)$ time.
This procedure was inspired by CHESS~\cite{ishaq2019clustered}, though we improved upon their partition method.

\begin{algorithm} % enter the algorithm environment
\caption{Partition} % give the algorithm a caption
\label{alg:partition} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE $cluster$
    \STATE $k \leftarrow \lfloor \sqrt{|cluster.points|} \rfloor$
    \STATE $seeds \leftarrow k$ random points from $cluster.points$
    \STATE $c \leftarrow$ geometric median of $cluster.points$
    \STATE $r \leftarrow \argmax d(c,x) \ \forall \ x \in cluster.points$
    \STATE $l \leftarrow \argmax d(r,x) \ \forall \ x \in cluster.points$
    \STATE $cluster.left \leftarrow \{x | x \in cluster.points \land d(l,x) \le d(r,x)\}$
    \STATE $cluster.right \leftarrow \{x | x \in cluster.points \land d(r,x) < d(l,x)\}$
    \IF{$|cluster.left| > 1$}
        \STATE Partition(cluster.left)
    \ENDIF
    \IF{$|cluster.right| > 1$}
        \STATE Partition(cluster.right)
    \ENDIF
\end{algorithmic}
\end{algorithm}

These clusters have several interesting and important properties for us to consider.
These include the \textit{cardinality}, the number of points in a cluster;
\textit{center}, the approximate geometric median of points contained in a cluster;
\textit{radius}, the distance to the farthest point from the center;
and \textit{local fractal dimension},
as given by:

\begin{gather}
    \log_2\bigg(\frac{|B_D(c, r)|}{|B_D(c, \frac{r}{2})|}\bigg)
    \label{fractal-dimension}
\end{gather}

where $B_D(c,r)$ is the set of points contained in a ball on the dataset $D$ of radius $r$ centered on a point $c$~\cite{ishaq2019clustered}.
Thus, local fractal dimension captures the ``spread'' of points on the manifold in comparison to the (typically much larger) embedding space.
This is motivated by the idea that the induced graphs will learn to adapt to use different ``resolutions'' to characterize different regions of the manifold.

We can also consider individual \textit{parent-child ratios} of cardinality, radius, and local fractal dimension, as well as the \textit{exponential moving averages} of those parent-child ratios along a branch of the tree.
In particular, we use the parent-child ratios and the exponential moving averages of those ratios to generalize our method from a small set of training datasets to a large, distinct set of testing datasets.
During clustering, we memoize these ratios as we create each cluster.
CHAODA can then make direct use of these ratios to aid in anomaly detection.


\subsection{Graphs}
\label{subsec:methods:graphs}

Clusters that are close together sometimes have overlapping volumes; i.e.,\ the distance between their centers is less than or equal to the sum of their radii.
We define a graph $G=(V,E)$ with clusters in one-to-one correspondence to vertices and with an edge between two vertices if and only if their corresponding clusters overlap.
While it is fairly standard in the literature to define graphs using this geometric property of overlapping volumes, the challenge is in selecting the right clusters to build useful graphs.
Our selection process, presented in (cite subsection), is among the major novel contributions of CLAM and CHAODA.

In the context of graphs, we use \textit{cluster} and \textit{vertex} interchangeably.
By \textit{graph cardinality} we mean \textit{vertex cardinality}; i.e.,\ the number of clusters in the graph, and by \textit{graph population} we mean the sum of cardinalities of all clusters in the graph.
We use \textit{layer-graph} to refer to a graph built from clusters at a fixed depth from the tree, and \textit{optimal-graph} to refer to a graph built from clusters selected be the processes described in (cite subsection).

Figure~\ref{fig:methods:graph-generation} illustrates how CLAM induces a graph from non-uniform depths in a cluster tree.
Interestingly, the clusters are not necessarily hyperspheres, but polytopes akin to a high-dimensional Voronoi diagram~\cite{voronoi1908nouvelles}.
The induced graph need not be fully connected, and in practice often contains many small, disjoint connected components.

For our purposes, a graph exhibits an important invariant.
The clusters corresponding to vertices in the graph collectively contain every point in the dataset and each point in the dataset is contained in, i.e.\ assigned to, exactly one cluster in the graph.
A corollary to this invariant is that a graph will never contain two clusters such that one cluster is an ancestor or descendant of another cluster.
This also assures that \textit{graph population} is equal to the cardinality of the dataset, i.e. $|\textbf{X}|$.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=6in]{images/chaoda-workflow.pdf}
    \caption{Overview of the CHAODA workflow.
        Blah blah. Say something about arbitrary color scheme, and note e.g. similarity between degree and stationary distribution. Walk through flow.}
    \label{fig:methods:chaoda-workflow}
\end{figure*}


% \subsection{Induced Graphs}
% \label{subsec:methods:induced-graphs}

% The heart of the problem with CHAODA is building the right graph(s) to represent the underlying manifold.
% One could try using every possible combination of clusters to form every possible graph but this leads to combinatorial explosion.
% Instead, we must intelligently select those clusters that build a graph which will perform well for anomaly detection.

% Area under the curve (AUC) of the receiver operating characteristic (ROC) is often used to measure the performance of anomaly detectors;
% we wish to choose clusters that are expected to maximize this measure.
% We use the definition of AUC-ROC based on CHAODA being an \emph{unsupervised} model where ground truth can be examined post-hoc from the datasets; a score is given to every point, and the AUC is computed in the usual fashion~\cite{fawcett2006introduction}.
% The ground-truth labels are only used during training for the training set of datasets, while the ground-truth for the test set of datasets is used purely for post-hoc performance calculations.
% Thus, CHAODA is supervised only in the sense of learning where to build a graph on training datasets, but unsupervised on any new (e.g. test) datasets.

% We find the ``right graphs'' by learning a function that takes the parent-child ratios of cardinality, radius and local fractal dimension and the exponential moving averages (from the root to the cluster) of these ratios for a cluster and predicts the contribution to AUC-ROC of selecting that cluster for the graph.
% We thus need to learn a function of the form $f: \mathbb{R}^6 \mapsto \mathbb{R}^+$ that takes these six ratios learn the AUC from the scores assigned to points in the corresponding cluster.
% By using these parent-child ratios and exponential moving averages thereof, we gain some agnosticity to dataset-specific properties and instead learn geometric and topological properties that generalize over datasets in Banach Spaces that obey the Manifold Hypothesis.
% This is essentially transfer learning: we will learn such functions from a set of datasets \emph{completely distinct from} the set of datasets we use for benchmarking.
% We are effectively learning a set of criteria that can enable selection of a useful depth (perhaps not truly optimal), varying along the cluster tree, with regards to anomaly detection.

% We choose a linear regression model and a regression tree model to fill this role.
% Such models require training data.
% To generate this data, we take a random sample from the datasets described in Section~\ref{subsec:methods:datasets}, choosing six datasets at random with between 1,000 and 100,000 points.
% We generate a clustering for each dataset, and start by inducing several uniform-depth (layer) graphs from the tree.
% We apply each individual algorithm described in Section~\ref{subsec:methods:individual-algorithms} to each such graph and, using the labels, we calculate the AUC over the subset of points in each cluster.
% We use that AUC as the training target for our meta-ml models.
% The corresponding feature vector is, as stated earlier, the six-element vector of the parent-child ratios and their exponential moving averages.
% This concludes the first epoch of training and gives us a set of trained meta-ml models.
% For each subsequent epoch of training, we use the meta-ml models from the previous epoch to rank the clusters in each tree (corresponding to each training dataset) and select graphs that perform better, as measured by AUC-ROC, for anomaly detection.
% Each epoch, thus, iteratively improves the meta-ml models used to select graphs and each newly selected graph produces better data for the meta-ml models to be used in the next epoch.

% Once we are satisfied with the performance of our meta-ml models on the set of training datasets, we freeze the meta-ml models and use them to select graphs for the test set of datasets.
% Since the ratios used to train and predict from these meta-ml models are agnostic to dataset specific properties such as cardinality and dimensionality, the trained models transfer from the training set of datasets to the \textit{completely distinct} test set of datasets.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=2.5in]{images/tree-graph.pdf}
    \caption{Using CLAM to induce a graph from a cluster tree.
        Dots in the tree represent cluster centers;
        blue dots represent centers of chosen clusters.
        Circles represent the volume of a cluster (the radius is the distance from the center to the furthest point contained within that cluster).
        Gray arrows point to the induced graph components, which are indicated in blue below the horizontal line.}
    \label{fig:methods:graph-generation}
\end{figure}


\subsection{Individual Algorithms}
\label{subsec:methods:individual-algorithms}

Having induced several graphs to characterize a manifold, we need to come up with some methods to extract information about the anomalousness of clusters in those graphs.
Here we describe six simple algorithms for anomaly detection, each using a CLAM graph to calculate an anomalousness score for each cluster and datum.
With the key to an effective ensemble method being that each component algorithm contribute a unique inductive bias~\cite{chen2017outlier}, we also note the intuition behind their contributions to the ensemble.

In the following,
$V$ and $E$ are the sets of clusters and edges, respectively, in a graph,
$|c|$ is the cardinality of a cluster $c$,
and $|C|$ is the cardinality of a component $C$.
Each algorithm assigns an anomalousness score to each cluster.
Each point is then assigned the anomalousness score of the cluster it belongs to.
These scores are internally consistent for each individual algorithm, i.e.\ low scores indicate inliers and high scores indicate outliers.
However, different algorithms assign scores in wide, and often different, ranges of values.
We use gaussian normalization constrain the raw scores to a $[0, 1]$ range.
This lets us combine scores into an ensemble (refer subsection).
See (cite normalization paper) for a more thorough discussion of anomaly-score normalization in ensemble methods.

\subsubsection{Relative Cluster Cardinality}
\label{subsubsec:methods:individual-algorithms:relative-cluster-cardinality}
We measure the anomalousness of point by the cardinality of the cluster that the point belongs to relative to the cardinalities of the other clusters in the graph.
Points in the same cluster are considered equally anomalous.
Points in clusters with lower cardinalities are considered more anomalous than points in clusters with higher cardinalities.

The intuition here is that points in clusters with higher cardinalities are all close to each other and are, thus, likely to be inliers, i.e.\ not anomalous.
The algorithm is defined in Algorithm~\ref{alg:relative-cluster-cardinality} and its time complexity is $\mathcal{O}(|V|)$ because it requires a single pass over the clusters in a graph.

\begin{algorithm}[h]
    \caption{Relative Cluster Cardinality}
    \label{alg:relative-cluster-cardinality}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {cluster $c \in G$}
    \STATE $scores[c] \gets -|c|$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Relative Component Cardinality}
\label{subsubsec:methods:individual-algorithms:relative-component-cardinality}
We use the usual definition of connected components:
no two nodes from different components have an edge between them, and
every pair of nodes in the same component has a path connecting them.
Consider the relative cardinalities of each component in much the same way as in the previous algorithm.
Points in clusters in smaller components are considered more anomalous than those in clusters in larger components.
Points in clusters in the same component are considered equally anomalous.

The intuition here, as distinct from the previous algorithm, is to capture larger-scale structural information based on disjoint connected components from the graph.
The algorithm is defined in Algorithm~\ref{alg:relative-component-cardinality}.
The time complexity is $\mathcal{O}(|E| + |V|)$ because we first need to find the components of the graph, using a single pass over the edges, and then score each cluster in the graph, using a single pass over those clusters.

\begin{algorithm}[h]
    \caption{Relative Component Cardinality}
    \label{alg:relative-component-cardinality}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {component $C \in G$}
        \STATE $scores[C] \gets -|C|$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Graph Neighborhood Size}
\label{subsubsec:methods:individual-algorithms:graph-neighborhood-size}
Given the graph, we consider the number of clusters reachable from a starting cluster within a given graph distance $k$, i.e.\ within $k$ hops along edges.
We call this number the \textit{graph-neighborhood size} of the starting cluster.
With $k$ small compared to the diameter of the graph, we consider the relative graph-neighborhood-sizes of the clusters.
Clusters with small graph-neighborhoods are considered more anomalous than clusters with large graph-neighborhoods.

The intuition here is to capture information about the connectivity of the graph in the region around each cluster.
The algorithm is defined in Algorithm~\ref{alg:graph-neighborhood-size}.
Its time complexity is $\mathcal{O}(|E| \cdot |V|)$ because we need to compute the eccentricity of each cluster.

\begin{algorithm}[h]
    \caption{Graph Neighborhood}
    \label{alg:graph-neighborhood-size}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \REQUIRE $f \in \mathbb{R}$ in the range $(0,1]$ ($0.25$ by default).
    \FOR {cluster $c \in G$}
        \STATE $e_c \gets$ the eccentricity of $c$
        \STATE $s \gets e_c \cdot f$
        \STATE perform a breadth-first traversal from $c$ with $s$ steps
        \STATE $v \gets$ the number of unique clusters visited by the traversal
        \STATE $scores[c] \gets -v$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Child-Parent Cardinality Ratio}
\label{subsubsec:methods:individual-algorithms:child-parent-cardinality-ratio}
As described in Section~\ref{subsec:methods:clustering}, the partition algorithm used in clustering splits a cluster into two children.
If a child-cluster contains only a small fraction of its parent's points, then we consider that child-cluster to be more anomalous.
These child-parent cardinality ratios are accumulated along each branch in the tree, terminating when the child-cluster is among those selected in the graph.
Clusters with a low value of these accumulated ratios are considered more anomalous than clusters with a higher value.

This algorithm was inspired by iForest~\cite{tony2008iforest}, and captures information from the tree and from the graph.
It is defined in Algorithm~\ref{alg:child-parent-cardinality-ratio}.
Unlike the other individual algorithms, this one accumulates parent scores into the children.
The time complexity of this algorithm is $\mathcal{O}(|V|)$ because these ratios are memoized during the clustering process.

\begin{algorithm}[h]
    \caption{Child-Parent Cardinality Ratio}
    \label{alg:child-parent-cardinality-ratio}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {cluster $c \in G$}
        \STATE $p \gets$ the parent cluster of $c$
        \STATE $scores[c] \gets \frac{|p|}{|c|} + scores[p]$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Stationary Probabilities}
\label{subsubsec:methods:individual-algorithms:stationary-probabilities}
We take an induced graph and assign each edge a weight inversely proportional to the distance between the centers of the two clusters that connect to form that edge.
We compute the transition probability matrix of each component of the graph that contains at least two clusters.
These probabilities are stochastic over the edge weights for each cluster.
This process of successively squaring this matrix will converge so long as the graph is connected~\cite{levin2017markov}.
We follow this process for each component in the graph and find the convergent matrix.
Consider the sum of the values along a row in the convergent matrix.
This is the expected proportion of visits to that cluster during a long random walk over the graph component.
We consider this sum to be inversely related to the anomalousness of the corresponding cluster.

The intuition here is that clusters that are more difficult to reach during an infinite random walk are more likely to contain anomalous points.
The algorithm is defined in Algorithm~\ref{alg:stationary-probabilities}.
Its worst-case time complexity is $\mathcal{O}(|V|^{2.3728596})$ given by the matrix multiplication algorithm from~\cite{alman2021refined}.
In practice, however, this algorithm has much better performance because the induced graphs are often composed of several small components rather than one, or a few, large component(s).

\begin{algorithm}[h]
    \caption{Stationary Probabilities}
    \label{alg:stationary-probabilities}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {component $C \in G$}
        \STATE $M \gets$ the transition matrix for $C$
        \REPEAT
            \STATE $M \gets M^2$
        \UNTIL $M$ converges
        \FOR {cluster $c \in C$}
            \STATE $s \gets $ the row from $M$ corresponding to $c$
            \STATE $scores[c] \gets -\Sigma(s)$ 
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Relative Vertex Degree}
\label{subsubsec:methods:individual-algorithms:relative-vertex-degree}
For each cluster in the induced graph, consider its vertex degree, i.e. the number of edges connecting to that cluster.
We consider a cluster with a high degree to be less anomalous than a cluster with low degree.
This is essentially a version of the previous algorithm that ignores edge weights, and may have different biases with regard to the sampling density of the dataset.
Its time complexity is $\mathcal{O}(|V|)$.

\begin{algorithm}[h]
    \caption{Relative Vertex Degree}
    \label{alg:relative-vertex-degree}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {cluster $c \in G$}
    \STATE $scores[c] \gets -cluster.degree$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Cluster Selection for Graphs}
\label{subsec:methods:cluster-selection-for-graphs}
TODO

\subsection{Training Meta-ML Models}
\label{subsec:methods:training-meta-ml-models}
TODO


\subsection{The Ensemble}
\label{subsec:methods:the-ensemble}
TODO


\subsection{Datasets and Comparisons}
\label{subsec:methods:datasets-and-comparisons}
TODO


% \subsection{Ensemble}
% \label{subsec:methods:ensemble}

% Given a dataset and a list of distance functions for the dataset, we start by building a CLAM tree for each distance function on the dataset.
% We extract the parent-child ratios and the exponential moving averages of those ratios for each cluster in each tree.
% Given the set of meta-ml models learned from the training datasets as described in Section~\ref{subsec:methods:induced-graphs}, we use each model to rank every cluster.
% The highest ranked clusters from each tree are then used to build a graph.
% This gives us one graph for each combination of meta-ML model and distance function used.
% We apply each of the six methods described in~\ref{subsec:methods:individual-algorithms} on each such graph.
% With two meta-ML models (linear regression and regression trees) and two distance metrics ($L1$-norm and $L2$-norm), we have $24$ anomaly scores for each point in the dataset.
% These scores are then combined into an ensemble by simply taking the mean of all scores for each point.
% We present the AUC-ROC from this ensemble in Section~\ref{sec:results}.

% % The individual algorithms described in~\ref{subsec:methods:individual-algorithms} produce outlier scores for each cluster, rather than for each point.
% % We remedy this by assigning to each point the score of the cluster to which it belongs.
% % Since our graphs guarantee that each point is in exactly one cluster and that every point from the dataset is accounted for, this assigns an outlier score to each point in a dataset.
% Our individual methods produce scores with a wide range of values.
% The only common feature among these scores is that low scores correspond to inliers and high scores correspond to outliers.
% Therefore, we cannot directly compare scores across methods, which we need to be able to do for a functional ensemble.
% Thus, we normalize the scores to the range $[0, 1]$, using gaussian normalization~\cite{kriegel2011interpreting}.
% We include sigmoid and min-max normalization as options in our implementation.


% \subsection{Datasets}
% \label{subsec:methods:datasets}

% We sourced 24 datasets containing only numerical features from Outlier Detection Datasets (ODDS)~\cite{rayana2016odds}.
% All of these datasets were adapted from the UCI Machine Learning Repository (UCIMLR)~\cite{UCIMLR}, and were standardized by ODDS for benchmarks on anomaly and outlier detection.
% Note that CHAODA is able to handle entirely-categorical datasets, but not mixed datasets.

% We randomly select six datasets to train CHAODA: annthyroid, mnist, pendigits, satellite, shuttle, and thyroid.
% We benchmarked CHAODA 30 times on the test datasets, with different random seeds.
% We provide more details on the datasets in the Supplement.
% During testing, we noticed that even though we often see $|V| \ll n $, the graph neighborhood size and stationary probabilities methods from~\ref{subsec:methods:individual-algorithms} were prohibitive, so we only run them when $|V| < max(128, \lfloor \sqrt n \rfloor)$.
% We present these results in Table~\ref{table:results:test-performance} under the CHAODA-fast and CHAODA rows.
% CHAODA-fast exhibits comparable performance to CHAODA, so we set CHAODA-fast as the default in our implementation.
% All benchmarks were conducted on a 28-core Intel Xeon E5-2690 v4 2.60GHz, 512GB RAM and CentOS 7 Linux with kernel 3.10.0-1127.13.1.el7.x86\_64 \#1 SMP and Python 3.6.8.

% CHAODA, being an unsupervised algorithm, is only compared against other unsupervised algorithms.
% We compared against $18$ unsupervised algorithms collected in the pyOD suite~\cite{zhao2019pyod} and Scikit-Learn~\cite{pedregosa2011scikit}, as well as RS-Hash~\cite{sathe2016subspace}.
% A supervised version of CHAODA is possible future work which will open up comparisons against supervised or weakly-supervised methods such as DAGMM~\cite{zong2018deep}.


% \subsection{Notes}
% \label{subsec:methods:notes}

% % From intro:our-approach
% The space may also be defined by a distance \textit{function} that does not obey the triangle inequality, though we have not explored what difficulties this might introduce.


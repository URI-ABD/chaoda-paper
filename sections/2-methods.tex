\section{Methods}
\label{sec:methods}


\subsection{CLAM}
\label{subsec:methods:clam}

We present a manifold-mapping algorithm called CLAM (Clustered Learning of Approximate Manifolds).
As input, we need a dataset and a distance function for that dataset.
A dataset is a collection of $n$-points in a $D$-dimensional embedding space, $\textbf{X} = \{x_1 \dots x_n\}, x_i \in \mathbb{R}^D$.
A Distance Function takes two points in the dataset and deterministically produces a non-negative real number, $f : (\mathbb{R}^D, \mathbb{R}^D) \mapsto \mathbb{R}^+$.
The distance must be such that $f(x, x) = 0$ and $f(x, y) = f(y, x)$ $\forall x, y \in X$.
The distance function may or may not obey the triangle-inequality.
In this paper we used $L1$-norm and $L2$-norm, which are metrics.

\subsubsection{Clustering}
\label{subsubsec:methods:clam:clustering}

We start by building a divisive-hierarchical clustering of the data based on a random sampling of $\sqrt k$ points from the  $k$ points in a given cluster from the tree.
This achieves clustering in expected $\mathcal{O}(n \lg n)$ time, giving us a tree of clusters, with the root containing every point in the dataset, and each leaf containing a single point from the dataset.
The CLAM clustering algorithm is defined in Algorithm~\ref{alg:clam-partition}.
The procedure is inspired from~\cite{ishaq2019clustered}, although we have improved upon their partition method.
The root-cluster contains all points in the dataset and clusters are recursively partitioned until each leaf-cluster contains only one point.
% We can't cite our work as "us" during the review process

\begin{algorithm} % enter the algorithm environment
\caption{Partition} % give the algorithm a caption
\label{alg:clam-partition} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE $cluster$
    \STATE $c \leftarrow$ geometric median of $cluster.points$
    \STATE $r \leftarrow \argmax d(c,x) \ \forall \ x \in cluster.points$
    \STATE $l \leftarrow \argmax d(r,x) \ \forall \ x \in cluster.points$
    \STATE $cluster.left \leftarrow \{x | x \in data \land d(l,x) \le d(r,x)\}$
    \STATE $cluster.right \leftarrow \{x | x \in data \land d(r,x) < d(l,x)\}$
    \IF{$|cluster.left| > 1$}
        \STATE Partition(cluster.left)
    \ENDIF
    \IF{$|cluster.right| > 1$}
        \STATE Partition(cluster.right)
    \ENDIF
\end{algorithmic}
\end{algorithm}

These clusters have several interesting and important properties for us to consider.
These include the \textit{cardinality}, the number of points in a cluster;
\textit{center}, the geometric median of points contained in a cluster;
\textit{radius}, the distance to the farthest point from the center;
and \textit{local fractal dimension}, as described in~\cite{ishaq2019clustered}.
We can also consider individual \textit{parent-child ratios} of cardinality, radius, and local fractal dimension, as well as the \textit{exponential moving averages} of those parent-child ratios along a branch of the tree.
Each of these properties is computed during the process of clustering, and cached as metadata for each cluster.
In particular, we use the parent-child ratios and the exponential moving averages of those ratios to generalize our method from a small set of datasets to a large, distinct set of datasets.

\subsubsection{Graphs}
Clusters that are close in the embedding space sometimes have overlapping volumes, i.e.\ the distance between their centers is less than or equal to the sum of their radii.
We define a graph $G=(V,E)$ with the selected clusters in one-to-one correspondence to vertices, and with an edge between two vertices if and only if their corresponding clusters overlap.
Note that any given datapoint will be a member of only one cluster at a given depth in the tree, even though it might exist inside other clusters' radii.
Thus, the clusters are not necessarily hyperspheres, but rather polytopes akin to a high-dimensional analog of a Voronoi diagram~\cite{voronoi1908nouvelles}.
Figure~\ref{fig:methods:graph-generation} illustrates how a graph can be induced by clusters and their overlaps at different depths of a tree.

For our purposes, a graph exhibits an important invariant.
The clusters corresponding to vertices in the graph collectively contain every point in the dataset.
In addition, each point in the dataset is in exactly one cluster in the graph.
A corollary to this invariant is that a graph will never contain two vertices such that one vertex corresponds to an ancestor or descendent cluster of another vertex.
A Graph can be built from clusters at a fixed depth in the cluster-tree (a layer-graph), or from clusters from multiple different depths in the tree (an optimal-graph).
In this work we consider the cardinality of a graph to be \textit{vertex cardinality}, i.e.\ the number of vertices (clusters) in the graph.

\subsubsection{The Manifold}
According to the ``manifold hypothesis''~\cite{fefferman2016testing}, datasets collected from constrained generating processes that are embedded in a high-dimensional space typically only occupy a low-dimensional manifold in that space.

The graphs discussed thus far map this low-dimensional manifold in the original embedding space.
Different graphs do this at different levels of local or global resolution.
Our aim is to properly build such a graph, where different resolutions may be necessary for different regions of the manifold.
We can then apply several anomaly detection algorithms to these graphs.

We describe a set of algorithms in Section~\ref{subsec:methods:individual-algorithms}.
While mainly relying on graph information, these algorithms can also incorporate information from the tree, such as the Child-Parent Cardinality Ratio method described in Section~\ref{subsubsec:methods:individual-algorithms:cpcr}.
While these algorithms are themselves fairly simple, the real challenge is in selecting the right clusters for the graphs for these algorithms to operate upon.
We will demonstrate CLAM's manifold mapping to be effective enough that even these simple algorithms, more often than not, outperform state-of-the-art anomaly detection algorithms.


\subsection{Induced Graphs}
\label{subsec:methods:induced-graphs}

The heart of the problem with CHAODA is building the right graph(s) to represent the underlying manifold.
One could try using every possible combination of clusters to form every possible graph but this leads to combinatorial explosion.
Instead, we must intelligently select those clusters that build a graph which performs well for anomaly detection.

Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) is often used to measure the performance of anomaly detectors;
we wish to choose clusters that are expected to maximize this measure.
We do this by learning a function that takes a cluster and predicts its contribution to AUC if that cluster were selected for the graph.
Any function $f: Cluster \mapsto \mathbb{R}^+$ will suffice.
This is essentially a form a transfer learning: we will learn such functions from a set of datasets that is \emph{completely distinct from} the set of datasets we sue for benchmarking.
We are not learning any dataset-specific parameters, but instead the general geometric and topological properties common among datasets.

We chose a linear regression model and a regression tree model to fill this role.
Such models need some data to train with.
To generate this data, we took a random sample from the datasets described in Section~\ref{subsec:methods:datasets}, choosing six datasets at random which had between 1,000 and 100,000 points.
We generate CLAM manifolds for these training datasets, use the linear regression and regression tree models to learn from these datasets, and apply the results to an entirely different collection of datasets.
The process is as follows.

We generate CLAM trees for each training dataset.
We generate the initial training data by considering layer-graphs induced from these trees.
For each such graph, we calculate the arithmetic, geometric, and harmonic means of the parent-child cardinality ratio, radius ratio, and local fractal dimension ratio;
along with the exponential moving averages of these ratios, as described in Section~\ref{subsubsec:methods:clam:clustering}.
Each set of means forms a 6-dimensional feature vector, giving us three feature vectors for each graph.
For each individual algorithm described in Section~\ref{subsec:methods:individual-algorithms}, we apply it to the graph and obtain an AUC ROC\@.
We then train the linear regression and decision tree models to predict this AUC from the feature vectors extracted from the graph.
The result is a set of models for each individual algorithm of CHAODA, trained from the six training datasets;
the decision functions from the fitted models are then used to make predictions for all test datasets, regardless of any differences in dimensionality or other properties of those datasets.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=3in]{images/tree-graph.pdf}
    \caption{Cartoon illustration of how CLAM induces a graph from a cluster tree.
        Dots in the tree represent cluster centers;
        blue dots represent cluster centers chosen as graph vertices.
        Circles around those centers represent the volume of a cluster (the radius is the distance from the center to the furthest point contained within that cluster).
        Gray arrows point to the induced subgraphs, which are indicated in blue below the horizontal line.}
    \label{fig:methods:graph-generation}
\end{figure}


\subsection{Individual Algorithms}
\label{subsec:methods:individual-algorithms}

Here we describe several simple methods for anomaly detection, each of which uses a graph of clusters from CLAM to calculate an anomalousness score for each datapoint.
For each algorithm, $scores$ is a dictionary of clusters and their outlier scores,
$V$ is the set of clusters in a graph,
$E$ is the set of edges in a graph, and
the cardinality of a cluster $c$ is denoted by $|c|$, the number of points in that cluster.

\subsubsection{Relative Cluster Cardinality}
We measure the anomalousness of a point by the cardinality of the cluster that the point belongs to relative to the cardinalities of the other clusters in the graph.
Points in the same cluster are considered equally anomalous and points in clusters with lower cardinalities are considered more anomalous than points in clusters with higher cardinalities.
The algorithm is defined in Algorithm~\ref{alg:rclc}.
The time complexity of this algorithm is $O(|V|)$.

\begin{algorithm}[h]
    \caption{Relative Cluster Cardinality}
    \label{alg:rclc}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {cluster $c \in G$}
    \STATE $scores[c] \gets -|c|$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Relative Component Cardinality}
We use the usual definition of connected components:
no two nodes from different components have an edge between them, and
every pair of nodes in the same component has a path connecting them.
Consider the relative cardinalities of each component in much the same way as we considered the relative cardinalities of clusters in the Relative Cluster Cardinality method.
Points in clusters (vertices) in smaller components are considered more anomalous than points in clusters in larger components,
and points in clusters in the same component are considered equally anomalous.
The algorithm is defined in Algorithm~\ref{alg:rcc}.
This algorithm first finds the components of the graph, so its time complexity is $O(|E| + |V|)$.

\begin{algorithm}[h]
    \caption{Relative Component Cardinality}
    \label{alg:rcc}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {component $C \in G$}
        \STATE $scores[C] \gets -|C|$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Graph Neighborhood}
Given the graph with clusters and edges, consider the number of clusters reachable from a starting cluster within a given graph distance $k$.
We call this number the \textit{graph-neighborhood} of the starting cluster.
With $k$ small compared to the diameter of the graph, we consider the relative sizes of graph-neighborhoods of the clusters.
Points in clusters with small graph-neighborhoods are considered more anomalous than points in clusters with large graph-neighborhoods.
The algorithm is defined in Algorithm~\ref{alg:gns}.
This algorithm is dominated by computing the eccentricity of each cluster, so its time complexity is $O(|E| \cdot |V|)$.

\begin{algorithm}[h]
    \caption{Graph Neighborhood}
    \label{alg:gns}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \REQUIRE $f \in \mathbb{R}$ in the range $(0,1]$ ($0.25$ by default).
    \FOR {cluster $c \in G$}
        \STATE $e_c \gets$ the eccentricity of $c$
        \STATE $s \gets e_c \cdot f$
        \STATE perform a breadth-first traversal from $c$ with $s$ steps
        \STATE $v \gets$ the number of unique clusters visited by the traversal
        \STATE $scores[c] \gets -v$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Child-Parent Cardinality Ratio}
\label{subsubsec:methods:individual-algorithms:cpcr}
As described in Section~\ref{subsubsec:methods:clam:clustering}, the Partition algorithm splits a cluster into two child clusters.
Consider the fraction of points in a cluster that are assigned to each child.
If a child cluster contains only a small fraction of its parent's points, then we consider the points in that child cluster to be more anomalous.
These child-parent cardinality ratios are accumulated for each point down its branch in the tree, terminating when the child cluster is a node in the induced graph.
Points with a low value of these accumulated ratios are considered more anomalous than points with a higher value.
The algorithm is defined in Algorithm~\ref{alg:cpcr}.
The time complexity of calculating the ratios is $O(|V|)$ at clustering time, which is amortized over scoring the points;
the time complexity of scoring a point is $O(1)$.

% TODO: Make this faithful to the description in text. scores are accumulated down the tree.
\begin{algorithm}[h]
    \caption{Child-Parent Cardinality Ratio}
    \label{alg:cpcr}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {cluster $c \in G$}
        \STATE $p \gets$ the parent cluster of $c$
        \STATE $scores[c] \gets \frac{|p|}{|c|}$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Random Walks}
We perform long random walks on each component of a graph, and we count the number of times that each cluster was visited.
We consider the relative visitation counts among the clusters to be inversely related to their anomalousness.
The least visited clusters are the most anomalous, and the most visited clusters are the least anomalous.
The algorithm is defined in Algorithm~\ref{alg:rw}.
This algorithm is dominated by the computation of the transition matrix for the graph.
Its worst-case time complexity is $O(|V|^2)$, but is often better because the transition matrix is computed separately for each component in the graph.

\begin{algorithm}[h]
    \caption{Random Walks}
    \label{alg:rw}
\begin{algorithmic}[1]
    \REQUIRE $G = (V,E)$, a graph
    \STATE Initialize $v[c] \gets 0 \ \forall c \in G$
    \FOR {cluster $c \in G$}
        \STATE $c' \gets c$
        \FOR{$i=1$ {\bfseries to} $10 \cdot |V|$}
            \STATE $c' \gets n(d),$ a random neighbor of $d$
            \STATE increment $v[c']$
        \ENDFOR
    \ENDFOR
    \FOR {cluster $c \in G$}
        \STATE $scores[c] \gets -v[c]$
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Stationary Probabilities}
We compute the transition probability matrix of each component of a graph that contains at least two clusters.
We then compute successive squares of this matrix.
This process will eventually converge as long as the graph is connected and aperiodic~\cite{levin2017markov}, and we find this convergent matrix.
We consider the sum of the values along a row to be inversely related to the anomalousness of the respective cluster.
The algorithm is defined in Algorithm~\ref{alg:sp}.
This algorithm is dominated by the computation of a separate transition matrix for each component in the graph.
Its worst-case time complexity is $O(|V|^2)$, but is often better in practice.

\begin{algorithm}[h]
    \caption{Stationary Probabilities}
    \label{alg:sp}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {component $C \in G$}
        \STATE $M \gets$ the transition matrix for $C$
        \REPEAT
            \STATE $M \gets M^2$
        \UNTIL $M$ converges
        \FOR {cluster $c \in C$}
            \STATE $s \gets $ the row from $M$ corresponding to $c$
            \STATE $scores[c] \gets -\Sigma(s)$ 
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Normalization}
\label{subsec:methods:normalization}

The individual methods described in~\ref{subsec:methods:individual-algorithms} produce outlier scores for each cluster, rather than for each point.
We remedy this by assigning each point the score of the cluster it belongs to.
Since our graphs guarantee that each point is in exactly one cluster, and that every point from the dataset is accounted for, this assigns an outlier score to each point in a dataset.

However, the individual methods still produce scores in a wide range of values.
The only common feature among these scores is that low scores correspond to inliers and high scores correspond to outliers.
Therefore, we cannot directly compare scores from different methods, which is needed for ensemble methods.
We normalize to the range $[0, 1]$, using gaussian normalization by default as described in~\ref{alg:normalization}.
We include sigmoid normalization and min-max normalization as options in our implementation.

\begin{algorithm}[h]
    \caption{Gaussian Normalization}
    \label{alg:normalization}
\begin{algorithmic}[1]
    \REQUIRE $X$, a dataset
    \REQUIRE $S$, a set of outlier scores for each point in $X$
    \STATE $erf: x \mapsto \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-u^2} \,du $
    \STATE $\mu \gets mean(S)$
    \STATE $\sigma \gets std(S)$
    \FOR {point $p \in X$}
        \STATE $S[p] \gets \frac{1}{2} \Big( 1 + erf \big(\frac{S[p] - \mu}{\sigma \cdot \sqrt{2}}\big) \Big) $
    \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{Ensemble}
\label{subsec:methods:ensemble}

Given a dataset and a list of distance functions for the dataset, we start by building a CLAM tree for each distance function on the dataset.
We extract the parent-child ratios and the exponential moving averages of those ratios for each cluster in each tree.
Given the set of models learned from the training datasets described in Section~\ref{subsec:methods:induced-graphs}, we use each model to rank every cluster, normalized by cardinality.
The highest ranked clusters from each tree are then used to build a graph.
This gives us one graph for each combination of individual algorithm, meta-ML model, and distance function used.
We used the six individual algorithms described in~\ref{subsec:methods:individual-algorithms}, three means (arithmetic, geometric, and harmonic), two meta-ML models (linear regression and regression trees), and two distance metrics ($L1$-norm and $L2$-norm);
thus, we have upto $72$ graphs for each dataset.

Each graph is then used with its corresponding individual algorithm to calculate an outlier score for each point in the dataset.
These outlier scores are then combined into an ensemble by simply taking the mean of all the scores for each point.
We present the AUC scores from this ensemble in Section~\ref{sec:results}.

\subsubsection{A Note on Runtime Performance}

During testing, we noticed that even though we often see $|V| \ll n $, where $n$ is the number of points in the dataset and $V$ is the set of vertices in a graph, the more expensive methods from~\ref{subsec:methods:individual-algorithms} took too long to run.
We remedy this by only running the expensive algorithms, i.e.\ Graph Neighborhood Size, Random Walks, and Stationary Probabilities, when $|V| < max(128, \lfloor \sqrt n \rfloor)$.
We compared the effect on AUC of using this threshold and present the results in Table~\ref{table:results:test-performance} under the CHAODA-fast and CHAODA rows.
CHAODA-fast exhibits negligible difference from CHAODA in performance, so we
set CHAODA-fast as the default in our implementation.


\subsection{Datasets}
\label{subsec:methods:datasets}

We sourced 24 datasets, all from Outlier Detection Datasets (ODDS)~\cite{rayana2016odds}, for training CHAODA and testing its performance.
All of these datasets were adapted from the UCI Machine Learning Repository (UCIMLR)~\cite{UCIMLR}, and were standardized, by ODDS, for benchmarks on anomaly and outlier detection.

Of these 24 datasets, we selected a random sample of 6 datasets to use for training CHAODA\@.
These training datasets were: annthyroid, mnist, pendigits, satellite, shuttle, and thyroid.
The remaining datasets were used to benchmark the performance of CHAODA\@.
Our performance on the test set is shown in Table~\ref{table:results:test-performance}.
We provide more details on the datasets, including their size, dimensionality, and fraction of outliers, in the supplementary materials.

Note that CHAODA is an unsupervised algorithm for outlier detection.
As such, we compare only against other unsupervised algorithms. %TODO but REPEN?
We compared against $18$ unsupervised algorithms collected in the pyOD suite~\cite{zhao2019pyod}, as well as (list others here)~\cite{}. % TODO fill this in. SKLearn
% looks like just loci in the table, and rs-hash for a couple datasets, is this right? if so, let's clarify here.
A supervised version of CHAODA will be possible future work.

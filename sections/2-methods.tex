\section{Methods}
\label{sec:methods}

% Overview
CLAM and CHAODA comprise many components, all described in this section.
Here, we give a brief overview of those components.
CLAM begins with a dataset and a distance metric, to which it applies hierarchical clustering to build a tree.
CLAM selects clusters from the tree using meta-ml models trained\footnote{Note that during training, CHAODA is somewhat supervised.
CHAODA uses this training to learn a set of meta-ml models for selecting clusters and inducing graphs.
During testing, and inference, on new datasets, CHAODA is unsupervised and uses the learned meta-ml models.} according to several geometric and topological properties.
These meta-ml models learn relationships between these properties and expected anomaly-detection performance.
CLAM then induces graphs from the selected clusters.
CHAODA applies its consituent algorithms to these graphs, and combines the individual scores into an ensemble, ultimately producing anomaly scores for each datum.


\subsection{Dataset and Distance Function}
\label{subsec:methods:dataset-and-distance-function}

We start with a dataset $\textbf{X} = \{x_1 \dots x_n\}$ with $n$ points and a distance function $f : (\textbf{X}, \textbf{X}) \mapsto \mathbb{R}^+$.
The distance function takes two points from the dataset and deterministically produces a non-negative real number.
We also require that the distance function is symmetric and that the distance between two identical points is zero, i.e.\ $\forall x, y \in X$, $f(x, y) = f(y, x)$ and $f(x, y) = 0 \Leftrightarrow x = y$.
CLAM and CHAODA are general over any distance function that obeys these constraints.

CLAM assumes the ``manifold hypothesis''~\cite{fefferman2016testing}, i.e.\ datasets collected from constrained generating phenomena that are embedded in a high-dimensional space typically only occupy a low-dimensional manifold in that space.
CLAM and CHAODA learn the geometric and topological properties of these manifolds in a way that generalizes across datasets and distance function regardless of dataset-specific properties such as total number of points, dimensionality, absolute distance values, etc.
We demonstrate this genericity by our anomaly detection performance in Section~\ref{sec:results}.

Note that we often speak of each datum as embedded in a $D$-dimensional Banach space and we use Euclidean notions, such as voids and volumes, to talk about the geometric and topological properties of the manifold.
The purpose of such notions is to help build intuition and aid understanding.
Mathematically, CLAM does not rely on such notions; in-fact, the details of an embedding space can be abstracted away behind the distance function.

Also note that we can provide certain guarantees (see CHESS~\cite{ishaq2019clustered}) when the distance function is a metric, i.e.\ it obeys the triangle inequality.
While CLAM and CHAODA work well with distance functions that are not metrics, we have not investigated how the lack of the triangle inequality changes, or breaks, those guarantees in the context of anomaly detection.
For this paper, we show results using the $L1$-norm and $L2$-norm.


\subsection{Clustering}
\label{subsec:methods:clustering}

We start by building a divisive hierarchical clustering of the dataset.
We partition, as described in Algorithm~\ref{alg:partition}, a cluster with $k$ points using a pair of well-separated points from among a random sampling of $\sqrt k$ points.
Starting from a root-cluster containing the entire dataset, we continue until each leaf contains only one datum.
This achieves clustering in expected $\mathcal{O}(n \lg n)$ time.
This procedure improves upon the clustering approach from CHESS~\cite{ishaq2019clustered} by a better selection of maximally-separated points, and by memoizing critical information about each cluster.

\begin{algorithm} % enter the algorithm environment
\caption{Partition} % give the algorithm a caption
\label{alg:partition} % and a label for \ref{} commands later in the document
\begin{algorithmic}[1] % enter the algorithmic environment
    \REQUIRE $cluster$
    \STATE $k \leftarrow \lfloor \sqrt{|cluster.points|} \rfloor$
    \STATE $seeds \leftarrow k$ random points from $cluster.points$
    \STATE $c \leftarrow$ geometric median of $seeds$
    \STATE $r \leftarrow \argmax d(c,x) \ \forall \ x \in cluster.points$
    \STATE $l \leftarrow \argmax d(r,x) \ \forall \ x \in cluster.points$
    \STATE $left \leftarrow \{x | x \in cluster.points \land d(l,x) \le d(r,x)\}$
    \STATE $right \leftarrow \{x | x \in cluster.points \land d(r,x) < d(l,x)\}$
    \IF{$|left| > 1$}
        \STATE Partition($left$)
    \ENDIF
    \IF{$|right| > 1$}
        \STATE Partition($right$)
    \ENDIF
\end{algorithmic}
\end{algorithm}

These clusters have several interesting and important properties for us to consider.
These include the \textit{cardinality}, the number of points in a cluster;
\textit{center}, the approximate geometric median of points contained in a cluster;
\textit{radius}, the distance to the farthest point from the center;
and \textit{local fractal dimension},
as given by:

\begin{gather}
    \log_2\bigg(\frac{|B_X(c, r)|}{|B_X(c, \frac{r}{2})|}\bigg)
    \label{fractal-dimension}
\end{gather}

where $B_X(c,r)$ is the set of points contained in a ball of radius $r$ on the dataset $X$ centered on a point $c$~\cite{ishaq2019clustered}.
Thus, local fractal dimension captures the ``spread'' of points on the manifold in comparison to the (typically much larger) embedding space.
This is motivated by the idea that the induced graphs will learn to adapt to use different ``resolutions'' to characterize different regions of the manifold.
% might want to reference "chicken's foot" figure here

We can also consider \textit{child-parent ratios} of cardinality, radius, and local fractal dimension of a cluster, as well as the \textit{exponential moving averages} of those child-parent ratios along a branch of the tree.
In particular, we use the child-parent ratios and the exponential moving averages of those ratios to generalize CHAODA from a small set of training datasets to a large, distinct set of testing datasets.
During clustering, we memoize these ratios as we create each cluster.
CHAODA can then make direct use of these ratios to aid in anomaly detection.


\subsection{Graphs}
\label{subsec:methods:graphs}

Clusters that are close together sometimes have overlapping volumes; i.e.,\ the distance between their centers is less than or equal to the sum of their radii.
We define a graph $G=(V,E)$ with vertices in one-to-one correspondence to CLAM clusters and with an edge between two vertices if and only if their corresponding clusters overlap.
While it is fairly standard in the literature to define graphs in this way, the challenge lies in selecting the right clusters to build useful graphs.
Our selection process, presented in Section~\ref{subsec:methods:cluster-selection-for-graphs}, is among the major novel contributions of CLAM and CHAODA.

In the context of graphs, we use the terms \textit{cluster} and \textit{vertex} interchangeably.
By \textit{graph cardinality} we mean \textit{vertex cardinality}; i.e.,\ the number of clusters in the graph, and by \textit{graph population} we mean the sum of cardinalities of all clusters in the graph.
Note that \textit{cluster cardinality} refers to the number of datapoints within a cluster.
%TODO might not need graph population.
We use \textit{layer-graph} to refer to a graph built from clusters at a fixed depth from the tree, and \textit{optimal-graph} to refer to a graph built from clusters selected by the processes described in Section~\ref{subsec:methods:cluster-selection-for-graphs}.

Figure~\ref{fig:methods:graph-generation} illustrates how CLAM induces a graph from non-uniform depths in a cluster tree.
Interestingly, the clusters are not necessarily hyperspheres, but polytopes akin to a high-dimensional Voronoi diagram~\cite{voronoi1908nouvelles}.
The induced graph need not be fully connected and, in practice, often contains many small, disjoint connected components.

For our purposes, a CLAM graph exhibits an important invariant.
The clusters corresponding to vertices in the graph collectively contain every point in the dataset and each point in the dataset is contained in, i.e.\ assigned to, exactly one cluster in the graph.
A corollary to this invariant is that a graph will never contain two clusters such that one cluster is an ancestor or descendant of another cluster.
This also assures that \textit{graph population} is equal to the cardinality of the dataset, i.e. $|\textbf{X}|$ or $n$.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=6in]{images/chaoda-workflow.pdf}
    \caption{\textbf{Overview of the CHAODA workflow.}
        Beginning with a dataset and a distance metric, CLAM builds a cluster tree and induces several graphs from this tree; for the sake of simplicity, we illustrate only one such graph here.
        Each of CHAODA's constituent algorithms provides distinct anomaly scores on its graph.
        These scores are normalized and aggregated into a final score for each cluster, and by extension, each datum.
        In this figure, we have simplified the scores to a ternary color scheme; actual scores are real-valued between 0 and 1.
        Note that each algorithm provides its own scoring, but there may be similarities such as between vertex degree and stationary distribution.}
    \label{fig:methods:chaoda-workflow}
\end{figure*}


% \subsection{Induced Graphs}
% \label{subsec:methods:induced-graphs}

% The heart of the problem with CHAODA is building the right graph(s) to represent the underlying manifold.
% One could try using every possible combination of clusters to form every possible graph but this leads to combinatorial explosion.
% Instead, we must intelligently select those clusters that build a graph which will perform well for anomaly detection.

% Area under the curve (AUC) of the receiver operating characteristic (ROC) is often used to measure the performance of anomaly detectors;
% we wish to choose clusters that are expected to maximize this measure.
% We use the definition of AUC-ROC based on CHAODA being an \emph{unsupervised} model where ground truth can be examined post-hoc from the datasets; a score is given to every point, and the AUC is computed in the usual fashion~\cite{fawcett2006introduction}.
% The ground-truth labels are only used during training for the training set of datasets, while the ground-truth for the test set of datasets is used purely for post-hoc performance calculations.
% Thus, CHAODA is supervised only in the sense of learning where to build a graph on training datasets, but unsupervised on any new (e.g. test) datasets.

% We find the ``right graphs'' by learning a function that takes the parent-child ratios of cardinality, radius and local fractal dimension and the exponential moving averages (from the root to the cluster) of these ratios for a cluster and predicts the contribution to AUC-ROC of selecting that cluster for the graph.
% We thus need to learn a function of the form $f: \mathbb{R}^6 \mapsto \mathbb{R}^+$ that takes these six ratios learn the AUC from the scores assigned to points in the corresponding cluster.
% By using these parent-child ratios and exponential moving averages thereof, we gain some agnosticity to dataset-specific properties and instead learn geometric and topological properties that generalize over datasets in Banach Spaces that obey the Manifold Hypothesis.
% This is essentially transfer learning: we will learn such functions from a set of datasets \emph{completely distinct from} the set of datasets we use for benchmarking.
% We are effectively learning a set of criteria that can enable selection of a useful depth (perhaps not truly optimal), varying along the cluster tree, with regards to anomaly detection.

% We choose a linear regression model and a regression tree model to fill this role.
% Such models require training data.
% To generate this data, we take a random sample from the datasets described in Section~\ref{subsec:methods:datasets}, choosing six datasets at random with between 1,000 and 100,000 points.
% We generate a clustering for each dataset, and start by inducing several uniform-depth (layer) graphs from the tree.
% We apply each individual algorithm described in Section~\ref{subsec:methods:individual-algorithms} to each such graph and, using the labels, we calculate the AUC over the subset of points in each cluster.
% We use that AUC as the training target for our meta-ml models.
% The corresponding feature vector is, as stated earlier, the six-element vector of the parent-child ratios and their exponential moving averages.
% This concludes the first epoch of training and gives us a set of trained meta-ml models.
% For each subsequent epoch of training, we use the meta-ml models from the previous epoch to rank the clusters in each tree (corresponding to each training dataset) and select graphs that perform better, as measured by AUC-ROC, for anomaly detection.
% Each epoch, thus, iteratively improves the meta-ml models used to select graphs and each newly selected graph produces better data for the meta-ml models to be used in the next epoch.

% Once we are satisfied with the performance of our meta-ml models on the set of training datasets, we freeze the meta-ml models and use them to select graphs for the test set of datasets.
% Since the ratios used to train and predict from these meta-ml models are agnostic to dataset specific properties such as cardinality and dimensionality, the trained models transfer from the training set of datasets to the \textit{completely distinct} test set of datasets.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=2.5in]{images/tree-graph.pdf}
    \caption{Using CLAM to induce a graph from a cluster tree.
        Dots in the tree represent cluster centers;
        blue dots represent centers of chosen clusters.
        Circles represent the volume of a cluster (the radius is the distance from the center to the furthest point contained within that cluster).
        Gray arrows point to the induced graph components, which are indicated in blue below the horizontal line.}
    \label{fig:methods:graph-generation}
\end{figure}


\subsection{Individual Algorithms}
\label{subsec:methods:individual-algorithms}

Having induced several graphs to characterize a manifold, we must extract information about the anomalousness of clusters in those graphs.
Here we describe six simple algorithms for anomaly detection, each using a CLAM graph to calculate an anomalousness score for each cluster and datum.
Given that the key to an effective ensemble is that each member contributes a unique inductive bias~\cite{chen2017outlier}, we also note the intuition behind each algorithm's contributions.

In the following,
$V$ and $E$ are the sets of clusters and edges respectively in a graph,
$|c|$ is the cardinality of a cluster $c$,
and $|C|$ is the cardinality of a component $C$.
Each algorithm assigns an anomalousness score to each cluster.
Each point is then assigned the anomalousness score of the cluster it belongs to.
These scores are internally consistent for each individual algorithm, i.e.\ low scores indicate inliers and high scores indicate outliers.
However, different algorithms assign scores in wide, and often different, ranges of values.
We use gaussian normalization to constrain the raw scores to a $[0, 1]$ range.
This lets us combine scores into an ensemble (see Section~\ref{subsec:methods:the-ensemble}).
See~\cite{kriegel2011interpreting} for a thorough discussion of anomaly-score normalization in ensemble methods.

\subsubsection{Relative Cluster Cardinality}
\label{subsubsec:methods:individual-algorithms:relative-cluster-cardinality}
We measure the anomalousness of a point by the cardinality of the cluster that the point belongs to relative to the cardinalities of the other clusters in the graph.
Points in the same cluster are considered equally anomalous.
Points in clusters with lower cardinalities are considered more anomalous than points in clusters with higher cardinalities.
Formally, $\forall c \in G, \score(c) = -|c|$.

The intuition here is that points in clusters with higher cardinalities are all close to each other and are, thus, less likely to be anomalous.
The time complexity is $\mathcal{O}(|V|)$ because this requires a single pass over the clusters in a graph.

\subsubsection{Relative Component Cardinality}
\label{subsubsec:methods:individual-algorithms:relative-component-cardinality}
We use the usual definition of connected components:
no two vertices from different components have an edge between them, and
every pair of vertices in the same component has a path connecting them.
We consider points in clusters in smaller components to be more anomalous than those in clusters in larger components.
Points in clusters in the same component are considered equally anomalous.
Formally, $\forall C \in G, \forall c \in C, \score(c) = -|C|$.

The intuition here, as distinct from the previous algorithm, is to capture larger-scale structural information based on disjoint connected components from the graph.
The time complexity is $\mathcal{O}(|E| + |V|)$ because we first need to find the components of the graph, using a single pass over the edges, and then score each cluster in the graph, using a single pass over those clusters.

\subsubsection{Graph Neighborhood Size}
\label{subsubsec:methods:individual-algorithms:graph-neighborhood-size}
Given the graph, we consider the number of clusters reachable from a starting cluster within a given graph distance $k$, i.e.\ within $k$ hops along edges.
We call this number the \textit{graph-neighborhood size} of the starting cluster.
With $k$ small compared to the diameter of the graph, we consider the relative graph-neighborhood-sizes of all clusters.
Clusters with small graph-neighborhoods are considered more anomalous than clusters with large graph-neighborhoods.

The intuition here is to capture information about the connectivity of the graph in the region around each cluster.
The algorithm is defined in Algorithm~\ref{alg:graph-neighborhood-size}.
Its time complexity is $\mathcal{O}(|E| \cdot |V|)$ because we need to compute the eccentricity of each cluster.

\begin{algorithm}[h]
    \caption{Graph Neighborhood}
    \label{alg:graph-neighborhood-size}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \REQUIRE $\alpha \in \mathbb{R}$ in the range $(0,1]$ ($0.25$ by default).
    \FOR {cluster $c \in G$}
        \STATE $e_c \gets$ the eccentricity of $c$
        \STATE $s \gets e_c \cdot \alpha$
        \STATE perform a breadth-first traversal from $c$ with $s$ steps
        \STATE $v \gets$ the number of unique clusters visited by the traversal
        \STATE $\score(c) \gets -v$
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Child-Parent Cardinality Ratio}
\label{subsubsec:methods:individual-algorithms:child-parent-cardinality-ratio}
As described in Section~\ref{subsec:methods:clustering}, the partition algorithm used in clustering splits a cluster into two children.
If a child cluster contains only a small fraction of its parent's points, then we consider that child cluster to be more anomalous.
These child-parent cardinality ratios are accumulated along each branch in the tree, terminating when the child cluster is among those selected in the graph.
Clusters with a low value of these accumulated ratios are considered more anomalous than clusters with a higher value.
Formally, $\forall c \in G, \score(c) = \frac{|p|}{|c|} + score(p)$ where $p$ is the parent cluster of $c$.

This algorithm was inspired by iForest~\cite{tony2008iforest}, and captures information from the tree and from the graph.
Unlike the other individual algorithms, this one accumulates parent scores into the children.
The time complexity of this algorithm is $\mathcal{O}(|V|)$ because these ratios are memoized during the clustering process.


\subsubsection{Stationary Probabilities}
\label{subsubsec:methods:individual-algorithms:stationary-probabilities}
For each edge in the graph, we assign a probability inversely proportional to the distance between the centers of the two clusters that connect to form that edge.
The outgoing probabilities are stochastic over the edge weights for each cluster.
We compute the transition probability matrix of each component that contains at least two clusters.
The process of successively squaring this matrix will converge so long as the graph is connected~\cite{levin2017markov}.
We follow this process for each component in the graph and find the convergent matrix.
Consider the sum of the values along a row in the convergent matrix.
This is the expected proportion of visits to that cluster during aan infinitely long random walk over the component.
We consider this sum to be inversely related to the anomalousness of the corresponding cluster.

The intuition here is that clusters that are more difficult to reach during an infinite random walk are more likely to contain anomalous points.
The algorithm is defined in Algorithm~\ref{alg:stationary-probabilities}.
Its worst-case time complexity is $\mathcal{O}(|V|^{2.3728596})$ given by the matrix multiplication algorithm from~\cite{alman2021refined}.
In practice, however, this algorithm has much better performance than indicated by the theoretical complexity because the induced graphs are often composed of several small components rather than one, or a few, large component(s).

\begin{algorithm}[h]
    \caption{Stationary Probabilities}
    \label{alg:stationary-probabilities}
\begin{algorithmic}[1]
    \REQUIRE $G$, a graph
    \FOR {component $C \in G$}
        \STATE $M \gets$ the transition matrix for $C$
        \REPEAT
            \STATE $M \gets M^2$
        \UNTIL $M$ converges
        \FOR {cluster $c \in C$}
            \STATE $s \gets $ the row from $M$ corresponding to $c$
            \STATE $\score(c) \gets -\Sigma(s)$ 
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsubsection{Relative Vertex Degree}
\label{subsubsec:methods:individual-algorithms:relative-vertex-degree}
For each cluster in the induced graph, consider its vertex degree, i.e. the number of edges connecting to that cluster.
We consider a cluster with a high degree to be less anomalous than a cluster with low degree.
This is essentially a version of the previous algorithm that ignores edge weights, and may have different biases with regard to the sampling density of the dataset.
Formally, $\forall c \in G, \score(c) = -\deg(c)$.
Its time complexity is $\mathcal{O}(|V|)$.


\subsection{Training Meta-ML Models}
\label{subsec:methods:training-meta-ml-models}
TODO


\subsection{Cluster Selection for Graphs}
\label{subsec:methods:cluster-selection-for-graphs}

The heart of the problem with CHAODA is in selecting the "right" clusters that would build a graph that provides a useful representation of the underlying manifold.
One could try every possible combination of clusters to build graphs but this quickly leads to combinatorial explosion.
Instead, CHAODA focuses on intelligently selecting clusters for a graph which is expected to perform well for anomaly detection.
Area under the curve (AUC) of the receiver operating characteristic (ROC) is often used to benchmark anomaly detectors.
CHAODA selects clusters to optimize for this measure.

Specifically, we train a number of meta-ml models (see Section~\ref{subsec:methods:training-meta-ml-models} for details) and, from each model, we extract a function of the form $g : c \rightarrow \mathbb{R}$.
This function assigns high values to clusters which would increase ROC-AUC and low values to clusters which would decrease ROC-AUC.
As described in Algorithm~\ref{alg:cluster-selection}, the selection process begins by sorting, in non-increasing order, all clusters in the tree by the value assigned by $g$.
This sorting represents a ranking of clusters for expected anomaly detection performance.
We iteratively select the best cluster from the ranking and with each selection, we remove the ancestors and descendants of the selected cluster for the list of rankings.
Once the list of ranking is exhausted, we have selected the clusters to build an \textit{optimal graph}.

\begin{algorithm}[h]
    \caption{Cluster Selection}
    \label{alg:cluster-selection}
\begin{algorithmic}[1]
    \REQUIRE $T$, a cluster-tree
    \REQUIRE $g : c \rightarrow \mathbb{R}$ a ranking function
    \STATE $G \gets$ an empty graph
    \STATE $h \gets$ a list of all clusters $c \in T$ sorted by $g(c)$
    \REPEAT
        \STATE $c \gets$ pop\_first($h$)
        \STATE Add $c$ to $G$
        \STATE Remove all ancestors and descendants of $c$ from $h$
    \UNTIL $h$ is empty
\end{algorithmic}
\end{algorithm}


\subsection{The Ensemble}
\label{subsec:methods:the-ensemble}
TODO


\subsection{Datasets and Comparisons}
\label{subsec:methods:datasets-and-comparisons}
TODO


% \subsection{Ensemble}
% \label{subsec:methods:ensemble}

% Given a dataset and a list of distance functions for the dataset, we start by building a CLAM tree for each distance function on the dataset.
% We extract the parent-child ratios and the exponential moving averages of those ratios for each cluster in each tree.
% Given the set of meta-ml models learned from the training datasets as described in Section~\ref{subsec:methods:induced-graphs}, we use each model to rank every cluster.
% The highest ranked clusters from each tree are then used to build a graph.
% This gives us one graph for each combination of meta-ML model and distance function used.
% We apply each of the six methods described in~\ref{subsec:methods:individual-algorithms} on each such graph.
% With two meta-ML models (linear regression and regression trees) and two distance metrics ($L1$-norm and $L2$-norm), we have $24$ anomaly scores for each point in the dataset.
% These scores are then combined into an ensemble by simply taking the mean of all scores for each point.
% We present the AUC-ROC from this ensemble in Section~\ref{sec:results}.

% % The individual algorithms described in~\ref{subsec:methods:individual-algorithms} produce outlier scores for each cluster, rather than for each point.
% % We remedy this by assigning to each point the score of the cluster to which it belongs.
% % Since our graphs guarantee that each point is in exactly one cluster and that every point from the dataset is accounted for, this assigns an outlier score to each point in a dataset.
% Our individual methods produce scores with a wide range of values.
% The only common feature among these scores is that low scores correspond to inliers and high scores correspond to outliers.
% Therefore, we cannot directly compare scores across methods, which we need to be able to do for a functional ensemble.
% Thus, we normalize the scores to the range $[0, 1]$, using gaussian normalization~\cite{kriegel2011interpreting}.
% We include sigmoid and min-max normalization as options in our implementation.


% \subsection{Datasets}
% \label{subsec:methods:datasets}

% We sourced 24 datasets containing only numerical features from Outlier Detection Datasets (ODDS)~\cite{rayana2016odds}.
% All of these datasets were adapted from the UCI Machine Learning Repository (UCIMLR)~\cite{UCIMLR}, and were standardized by ODDS for benchmarks on anomaly and outlier detection.
% Note that CHAODA is able to handle entirely-categorical datasets, but not mixed datasets.

% We randomly select six datasets to train CHAODA: annthyroid, mnist, pendigits, satellite, shuttle, and thyroid.
% We benchmarked CHAODA 30 times on the test datasets, with different random seeds.
% We provide more details on the datasets in the Supplement.
% During testing, we noticed that even though we often see $|V| \ll n $, the graph neighborhood size and stationary probabilities methods from~\ref{subsec:methods:individual-algorithms} were prohibitive, so we only run them when $|V| < max(128, \lfloor \sqrt n \rfloor)$.
% We present these results in Table~\ref{table:results:test-performance} under the CHAODA-fast and CHAODA rows.
% CHAODA-fast exhibits comparable performance to CHAODA, so we set CHAODA-fast as the default in our implementation.
% All benchmarks were conducted on a 28-core Intel Xeon E5-2690 v4 2.60GHz, 512GB RAM and CentOS 7 Linux with kernel 3.10.0-1127.13.1.el7.x86\_64 \#1 SMP and Python 3.6.8.

% CHAODA, being an unsupervised algorithm, is only compared against other unsupervised algorithms.
% We compared against $18$ unsupervised algorithms collected in the pyOD suite~\cite{zhao2019pyod} and Scikit-Learn~\cite{pedregosa2011scikit}, as well as RS-Hash~\cite{sathe2016subspace}.
% A supervised version of CHAODA is possible future work which will open up comparisons against supervised or weakly-supervised methods such as DAGMM~\cite{zong2018deep}.


% \subsection{Notes}
% \label{subsec:methods:notes}

% % From intro:our-approach
% The space may also be defined by a distance \textit{function} that does not obey the triangle inequality, though we have not explored what difficulties this might introduce.


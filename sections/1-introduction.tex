\section{Introduction}
\label{sec:introduction}

Detecting anomalies and outliers from data is a well-studied problem in machine learning.
When data occupy easily-characterizable distributions, such as the Gaussian, the task is relatively easy:
one need only identify when a datum is sufficiently far from the mean.
However, in ``big data'' scenarios, where data can occupy high-dimensional spaces, anomalous behavior becomes harder to quantify.
If the data happen to be uniformly distributed, one can conceive of simple mechanisms, such as a one-class SVM, that would be effective in any number of dimensions.
However, real-world data are rarely distributed uniformly.
Instead, data often obey the ``manifold hypothesis''~\cite{fefferman2016testing}, occupying a low-dimensional manifold in a high-dimensional embedding space, similar to how a 2-d sheet of paper, once crumpled, occupies a 3-dimensional space.
Detecting anomalies in such a landscape is not easy.
Imagine trying to identify if a point within the sphere of crumpled paper were anomalous; identifying whether or not the point sits on the sheet or within a gap of the lower-dimensional manifold presents a challenge.

Anomalies (data that do not belong to a distribution) and outliers (data which represent the extrema of a distribution) are found in datasets for a variety of reasons.
In this manuscript, we treat anomalies and outliers interchangeably.
Errors in the measurement of data create anomalous records,
novel instances of data emerge over time,
or normal behavior may evolve towards the outliers.
Moreover, even if data collection is executed perfectly and shifts in behavior are accounted for, there is still the risk of adversarial attacks causing undesired behavior~\cite{elsayed2018adversarial}.

Modern algorithms designed to detect anomalous behavior fail for a variety of reasons, in particular when anomalies live close to, but not on, a complex manifold in high-dimensional space.
Our approach is designed to map these complex manifolds.
Here we briefly survey contemporary approaches to anomaly detection in order to provide the context needed to understand how our approach differs.


\subsection{Clustering-based Approaches}
\label{subsec:introduction:clustering-based-approaches}

Clustering refers to techniques for grouping points in a way that provides value.
This is generally done by assigning \textit{similar} points to the same cluster.
Given a clustering and a new point, one can estimate the anomalousness of that new point by measuring its distance to its nearest cluster(s).

There have been few advancements in clustering techniques over the past decade~\cite{wang2019progress}.
This may be explained by the poor performance, thus far, of clustering in high-dimensional space~\cite{zhang2013advancements}.

Distance-based clustering relies on some distance measure to partition data into several clusters.
Within this approach, the numbers or sizes of clusters are often predetermined: either user-specified, or randomly chosen~\cite{wang2019progress}.
Examples of distance-based clustering algorithms are
K-Means~\cite{macqueen1967some},
PAM~\cite{kaufman2009finding},
CLARANS~\cite{ng1994efficient} and
CLARA~\cite{kaufman2009finding}.

Hierarchical clustering methods build a tree, where points are allocated into leaves~\cite{wang2019progress}.
These trees can be created via bottom-up (agglomerative) or top-down (divisive)~\cite{agrawal1998automatic} clustering.
The high cost of all-pairs distance computations is often a drawback.
Examples of hierarchical clustering algorithms include
MST~\cite{zahn1971graph},
CURE~\cite{guha1998cure}, and
CHAMELEON~\cite{karypis1999hierarchical}.

Grid-based clustering works via segmenting the entire volume into a discrete number of cells, and then scanning those cells to find regions of high density.
The grid structure typically lets these methods scale well to larger datasets.
Some examples include
STING~\cite{wang1997sting},
Wavecluster~\cite{sheikholeslami2000wavecluster},
CLIQUE~\cite{agrawal1998automatic},
Clustering Based Local Outlier Factor (CBLOF)~\cite{he2003cblof}, and
Local Correlation Integral (LOCI)~\cite{papadimitriou2003loci}.


\subsection{Density-based Approaches}
\label{subsec:introduction:density-based-approaches}

Density-based clustering methods rely on finding regions of high point-density separated by regions of lower density.
These algorithms generally do not work well when data are sparse or uniformly distributed.
Some examples include
DBSCAN~\cite{ester1996density} and
DENCLUE~\cite{hinneburg1998efficient}.
In this paper, we compare against Local Outlier Factor (LOF)~\cite{breunig2000lof}.


\subsection{Graph-based Approaches}
\label{subsec:introduction:graph-based-approaches}

Graph-based methods build graph representations of the data by treating points, or collections of points, as nodes in a graph and drawing edges between them based on various rules.
In this paper, we compare against Connectivity-based Outlier Factor (COF)~\cite{tang2002cof}.


\subsection{Distance-based Approaches}
\label{subsec:related-works:distance-based-approaches}

Distance-based methods use direct distance comparisons and largely rely on k-Nearest Neighbors as their substrate~\cite{wang2019progress}.
They tend to use the following intuitions.
First, points with fewer than $p$ other points within some distance $d$ are outliers;
second, the $n$ points with the greatest distances to their $k^{th}-$nearest neighbor are outliers;
and finally, the $n$ points with the greatest average distance to their $k$ nearest neighbors are outliers.
We compare against
k-Nearest Neighbors (kNN)~\cite{ramaswamy2000efficient, fabrizio2002knn} and
Subspace Outlier Detection (SOD)~\cite{kriegel2009sod}.


\subsection{Deep Learning Approaches}
\label{subsec:related-works:deep-learning-approaches}
With the recent explosion in Deep Learning, several new methods for anomaly detection have been introduced~\cite{pang2021deep}.
While most such approaches are supervised or weakly-supervised, such as DAGMM~\cite{zong2018deep}, there have been some advances in unsupervised methods.
These typically use autoencoders~\cite{chen2017outlier}, such as RandNet~\cite{chandola2009anomaly}, or generative-adversarial-networks, such as MO-GAAL and SO-GAAL~\cite{liu2019generative}.
We compare against MO-GAAL, SO-GAAL, and two autoencoders from~\cite{chen2017outlier}.


\subsection{Other Approaches}
\label{subsec:introduction:other-appraoches}

There are several approaches to anomaly detection that do not fall neatly into any of the aforementioned categories.
These methods often rely on support vector machines, random forests, or histograms to detect outliers.
We compare against seven methods among these:
Histogram-Based Outlier Detection (HBOS)\cite{goldstein2012histogram},
Isolation-Forest (IFOREST)~\cite{tony2008iforest,tony2012iforest},
One-class Support Vector Machine (OCSVM)~\cite{sholkopf2001ocsvm},
Linear Model Deviation-base outlier Detection (LMDD)~\cite{arning1996linear},
Lightweight Online Detector of Anomalies (LODA)~\cite{pevny2016loda},
Minimum Covariance Determinant (MCD)~\cite{rousseeuw1999mcd,hardin2004mcd}, and
Subspace Outlier Detection (SOD)~\cite{kriegel2009sod}.


\subsection{Our Approach}
\label{subsec:introduction:chaoda}

With the term \emph{manifold learning} being largely synonymous with dimension reduction, we propose \emph{manifold mapping} to refer to approaches that study the the geometric and topological properties of manifolds in their original embedding spaces.
We introduce a novel technique, Clustered Learning of Approximate Manifolds (CLAM) for datasets in a Banach space~\cite{banach1929fonctionnelles} defined by a distance metric.
CLAM presupposes the manifold hypothesis and uses a divisive hierarchical clustering to build a map of the manifold occupied by the dataset.
CLAM then provides this manifold as input to a collection of anomaly detection algorithms, which we call CHAODA (Clustered Hierarchical Anomaly and Outlier Detection Algorithms).

CLAM extends CHESS~\cite{ishaq2019clustered} by adding memoized calculations of several geometric and topological properties of clusters that are useful to CHAODA, and does so in expected $\mathcal{O}(n \lg n)$ time.
While, in principle, we could have used any hierarchical clustering algorithm, these memoized calculations are not provided for by other clustering algorithms.
Other clustering algorithms have also suffered from problems such as:
an ineffective treatment of high dimensionality,
an inability to interpret results, and
an inability to scale to exponentially-growing datasets~\cite{agrawal1998automatic}.
CLAM, as we will demonstrate, largely resolves these issues.

CHESS was used to build a hierarchical clustering to a user-specific depth for the sole purpose of accelerating search.
CLAM, however, divisively clusters the data until each cluster contains only one datum.
Using the cluster-tree, CLAM induces graphs by mapping specific clusters to vertices of a graph, and drawing an edge between any two vertices whose corresponding clusters have overlapping volumes (i.e.\ the sum of their radii is greater than the distance between their centers).
Clusters can be selected from a fixed depth, or from heterogeneous depths based on properties such as their local fractal dimension, cardinality, radius, etc.
We can consider clusters at lower depths in the tree to be, in some sense, ``lower resolution'' than those at greater depths.
Inducing a graph across a variety of depths effectively maps a manifold with a variety of ``resolutions'', the intuition being that some regions of the manifold may have a higher density of points than others and, thus, graphs induced from clusters deeper in the tree may be more informative for those regions.

Having mapped a manifold by clustering and inducing graphs, we can start to analyze several properties of the clusters in the graphs.
For example; what are the relative cardinalities of the clusters in the graph, how connected are the clusters, how often is each cluster visited by a random walk?
CHAODA uses answers to such questions, among others, to build an ensemble approach to anomaly detection.

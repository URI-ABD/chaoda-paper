\section{Introduction}
\label{sec:introduction}

TODO

% Detecting anomalies and outliers from data is a well-studied problem in machine learning.
% When data occupy easily-described distributions, such as the Gaussian, the task is relatively easy: one need only identify when a datum is sufficiently far from the mean.
% However, in ``big data'' scenarios, where data can occupy high-dimensional spaces, anomalous behavior becomes harder to quantify.
% If the data happen to be uniformly distributed, one can conceive of simple mechanisms, such as a one-class SVM, that would be effective in any number of dimensions.
% However, real-world data are rarely distributed uniformly.
% Instead, data often obey the ``manifold hypothesis''~\cite{fefferman2016testing}, occupying a low-dimensional manifold in a high-dimensional embedding space.
% This low-dimensional manifold may weave itself through the high-dimensional space much like a crumpled sheet of paper does in 3-dimensional space.
% Detecting anomalies in such a landscape is not easy;
% in particular, correctly identifying an anomalous datum that sits within the gaps of a lower-dimensional manifold presents a challenge.

% Anomalies (data that do not belong to a distribution) and outliers (data which represent extrema of a distribution) can arise from many sources:
% errors in measurement or collection of data;
% novel, previously-unseen instances of data;
% normal behavior evolving into abnormal behavior;
% and adversarial attacks as inputs to machine-learning algorithms~\cite{elsayed2018adversarial}.
% Modern algorithms designed to detect anomalous behavior fail for a variety of reasons, in particular when anomalies live close to, but not on, a complex manifold in high-dimensional space.
% Our approach is designed to learn these complex manifolds.
% Here we briefly survey contemporary approaches to anomaly detection in order to provide the context needed to understand how our approach differs.

\subsection{Related Works}
\label{subsec:introduction:related-works}

TODO

\subsubsection{Clustering Based Local Outlier Factor (CBLOF)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof
% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.4242&rep=rep1&type=pdf
The CBLOF operator calculates the outlier score based on cluster-based local outlier factor.

CBLOF takes as an input the data set and the cluster model that was generated by a clustering algorithm.
It classifies the clusters into small clusters and large clusters using the parameters alpha and beta.
The anomaly score is then calculated based on the size of the cluster the point belongs to as well as the distance to the nearest large cluster.

Use weighting for outlier factor based on the sizes of the clusters as proposed in the original publication.
Since this might lead to unexpected behavior (outliers close to small clusters are not found), it is disabled by default.Outliers scores are solely computed based on their distance to the closest large cluster center.

By default, kMeans is used for clustering algorithm instead of Squeezer algorithm mentioned in the original paper for multiple reasons.
\cite{he2003cblof}

\subsubsection{Connectivity-Based Outlier Factor (COF)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof
% https://dl.acm.org/doi/10.5555/646420.693665
Connectivity-Based Outlier Factor (COF) COF uses the ratio of average chaining distance of data point and the average of average chaining distance of k nearest neighbor of the data point, as the outlier score for observations.
\cite{tang2002cof}

\subsubsection{Histogram-Based Outlier Detection (HBOS)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos
% https://www.semanticscholar.org/paper/Histogram-based-Outlier-Score-(HBOS)%3A-A-fast-Goldstein-Dengel/405bde43709582b0026c0fd6f0afe2c3c57f792e
Histogram-Based outlier detection (HBOS) is an efficient unsupervised method.
It assumes the feature independence and calculates the degree of outlyingness by building histograms.
\cite{goldstein2012hbos}

\subsubsection{Isolation-Forest Outlier Detector (IFOREST)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest
% https://dl.acm.org/doi/10.1145/2133360.2133363
% https://dl.acm.org/doi/10.1109/ICDM.2008.17
The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.
\cite{tony2012iforest}
\cite{tony2008iforest}

\subsubsection{k-Nearest Neighbors (kNN)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn
% https://webdocs.cs.ualberta.ca/~zaiane/pub/check/ramaswamy.pdf
% https://www.researchgate.net/profile/Clara_Pizzuti/publication/220699183_Fast_Outlier_Detection_in_High_Dimensional_Spaces/links/542ea6a60cf27e39fa9635c6/Fast-Outlier-Detection-in-High-Dimensional-Spaces.pdf
For an observation, its distance to its kth nearest neighbor could be viewed as the outlying score.
It could be viewed as a way to measure the density.
\cite{sridhar2000knn}
\cite{fabrizio2002knn}

\subsubsection{Linear Model Deviation-base outlier Detection(LMDD)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod-models-lmdd-module
% http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.40.9276
LMDD employs the concept of the smoothing factor which indicates how much the dissimilarity can be reduced by removing a subset of elements from the data-set.
\cite{arning1996lmdd}

\subsubsection{LOcal Correlation Integral (LOCI)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci
% http://www.cs.cmu.edu/~christos/courses/826-resources/FOILS3-pdf/610_LOCI.pdf
LOCI is highly effective for detecting outliers and groups of outliers ( a.k.a.micro-clusters), which offers the following advantages and novelties: (a) It provides an automatic, data-dictated cut-off to determine whether a point is an outlier—in contrast, previous methods force users to pick cut-offs, without any hints as to what cut-off value is best for a given dataset. (b) It can provide a LOCI plot for each point; this plot summarizes a wealth of information about the data in the vicinity of the point, determining clusters, micro-clusters, their diameters and their inter-cluster distances. None of the existing outlier-detection methods can match this feature, because they output only a single number for each point: its outlierness score.(c) It can be computed as quickly as the best previous methods
\cite{papadimitriou2003loci}

\subsubsection{Lightweight Online Detector of Anomalies (LODA)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda
% https://link.springer.com/article/10.1007/s10994-015-5521-0#citeas
\cite{pevny2016loda}

\subsubsection{Local Outlier Factor (LOF)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof
% https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf
The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.
\cite{breunig2000lof}

\subsubsection{Minimum Covariance Determinant (MCD)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd
% https://ucdavis.pure.elsevier.com/en/publications/outlier-detection-in-the-multiple-cluster-setting-using-the-minim
% https://www.researchgate.net/publication/2298225_A_Fast_Algorithm_for_the_Minimum_Covariance_Determinant_Estimator
Detecting outliers in a Gaussian distributed dataset using Minimum Covariance Determinant (MCD): robust estimator of covariance.

The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.

First fit a minimum covariance determinant model and then compute the Mahalanobis distance as the outlier degree of the data
\cite{rousseeuw1999mcd}
\cite{hardin2004mcd}

\subsubsection{One-class Support Vector Machine (OCSVM)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm
% https://www.researchgate.net/publication/220499623_Estimating_Support_of_a_High-Dimensional_Distribution
Estimate the support of a high-dimensional distribution.
\cite{sholkopf2001ocsvm}

\subsubsection{Subspace Outlier Detection (SOD)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod
% https://www.dbs.ifi.lmu.de/Publikationen/Papers/pakdd09_SOD.pdf
Subspace outlier detection (SOD) schema aims to detect outlier in varying subspaces of a high dimensional feature space. For each data object, SOD explores the axis-parallel subspace spanned by the data object’s neighbors and determines how much the object deviates from the neighbors in this subspace.
\cite{kriegel2009sod}

\subsubsection{Stochastic Outlier Selection (SOS)}
% https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos
% https://pure.uvt.nl/portal/files/1517370/Janssens_outlier_11-06-2013.pdf
% https://www.datascienceworkshops.com/blog/stochastic-outlier-selection/
SOS employs the concept of affinity to quantify the relationship from one data point to another data point. Affinity is proportional to the similarity between two data points. So, a data point has little affinity with a dissimilar data point. A data point is selected as an outlier when all the other data points have insufficient affinity with it.
\cite{janssens2012sos}

% Go over the history of anomaly detection, manifold learning, manifold mapping etc.
% Describe each of the methods we caompare against.
% Mention some methods we did not compare against and why.
% Mention other methods that can be adapted to anomaly detection e.g. DBSCAN.

% \subsection{Clustering-based Approaches}
% \label{subsec:related-works:clustering-based-approaches}

% Clustering refers to techniques for grouping points in a way that provides value.
% This is generally done by assigning \textit{similar} points to the same cluster.
% Given a clustering and a new point, one can estimate the anomalousness of the new point by measuring its distance to its nearest cluster.

% There have been few advancements in clustering techniques over the past decade~\cite{wang2019progress}.
% This may be explained by the poor performance of clustering in high-dimensional space~\cite{zhang2013advancements} thus far.
% The major approaches are as follows.

% Distance-based clustering relies on some distance measure to partition data into some number of clusters.
% Within this approach, the numbers and/or sizes of clusters are often predefined: either user-specified, or chosen at random~\cite{wang2019progress}.
% Some examples of distance-based clustering are:
% K-Means~\cite{macqueen1967some},
% PAM~\cite{kaufman2009finding},
% CLARANS~\cite{ng1994efficient} and
% CLARA~\cite{kaufman2009finding}.

% Hierarchical clustering methods utilize a tree-like structure, where points are allocated into leaf nodes~\cite{wang2019progress}.
% These tree-like structures can be created bottom-up (agglomerative clustering) or top-down (divisive clustering)~\cite{agrawal1998automatic}.
% A major drawback of these methods is the high cost of pairwise difference computations required to build each level of the tree.
% Examples of hierarchical clustering include
% MST~\cite{charles_zahn_graph_1971},
% CURE~\cite{guha1998cure} and
% CHAMELEON~\cite{karypis1999hierarchical}.

% Density-based clustering methods rely on finding regions of high point-density separated by regions of low point-density.
% These algorithms generally do not work well when data are sparse or uniformly distributed.
% Some examples of density-based clustering algorithms are
% DBSCAN~\cite{ester1996density} and
% DENCLUE~\cite{hinneburg1998efficient}.

% Grid-based clustering works via segmenting the entire space into a discrete number of cells, and then scanning those cells to find regions of high density.
% Utilizing a grid structure for clustering means that these algorithms typically scale well to larger datasets.
% Some examples of grid-based clustering include
% STING~\cite{wang1997sting},
% Wavecluster~\cite{sheikholeslami2000wavecluster}, and
% CLIQUE~\cite{agrawal1998automatic}.

% \subsection{Distanced-based Approaches}
% \label{subsec:related-works:distanced-based-approaches}

% Distance-based methods find anomalous points via distance comparisons.
% These methods largely employ k-Nearest Neighbors as their substrate~\cite{wang2019progress}.
% Distance-based approaches tend to use the following intuitions:
% points with fewer than $p$ other points within some distance $d$ are outliers;
% the $n$ points with the greatest distances to their $k^{th}-$nearest neighbor are outliers;
% or the $n$ points with the greatest average distance to their $k$ nearest neighbors are outliers.

% \subsection{CHAODA}
% \label{subsec:related-works:chaoda}

% In this paper we introduce a novel technique, Clustered Learning of Approximate Manifolds (CLAM).
% This approach uses divisive hierarchical clustering to learn a manifold in a Banach space~\cite{banach1929fonctionnelles} defined by a distance metric.
% In actuality, we do not require a metric.
% The space may be defined by a distance \textit{function} that does not obey the triangle inequality, though this is not always optimal.
% Given a learned approximate manifold, we can almost trivially implement several anomaly-detection algorithms.
% In this manuscript, we present a collection of five such algorithms implemented on CLAM: CHAODA (Clustered Hierarchical Anomaly and Outlier Detection Algorithms).

% The manifold learning component is derived from prior work, CHESS~\cite{ishaq2019entropy}, to accelerate approximate search on large high-dimensional datasets.
% CLAM begins by divisively clustering the data until each cluster contains only one datum.
% CLAM then delineates \textit{layers} of clusters at each depth in the tree.
% Each layer comprises all clusters that would have been leaf nodes if the tree building had been halted at the given depth.

% CLAM then builds a graph for each layer in the tree by creating edges between clusters that have overlapping volumes.
% This process effectively learns the manifold on which the data lie at various resolutions, given by the depth of the layer.
% This is analogous to a ``filtration'' in computational topology~\cite{carlsson2009topology}.
% Once we have learned a manifold, we can ask about the cardinality of various clusters at different depths, how connected a given cluster is, or even how often a cluster is visited by random walks on the manifold.

% We test our methods on 24 real-world datasets.
% The datasets span a wide variety of domains, each having a different quantity of anomalous data.
% We consider several different definitions of outliers and anomalies: \textbf{distance-based}, examining several classical distance-based definitions of outliers, relying on CLAM's use of distance to cluster data; \textbf{density-based}, examining the cardinality of clusters, under the hypothesis that clusters with lower cardinality are more likely to contain outliers; \textbf{graph-based}, examining several graph-theoretic methods for anomaly detection, given graphs constructed from layers of clusters.

% Historically, clustering approaches have suffered from several problems.
% The most common deficiencies are: the effective treatment of high dimensionality, the ability to interpret results, and the ability to scale to exponentially-growing datasets ~\cite{agrawal1998automatic}.
% CLAM largely resolves these problems.


\subsection{CHAODA}
\label{subsec:introduction:chaoda}

TODO

\section{Discussion}
\label{sec:discussion}

% TODO mention reviewer re: acknowledgements in camera ready
We have presented CHAODA, an ensemble of six algorithms that use the map of the underlying manifold produced by CLAM\@.
The six individual algorithms are simple to implement on top of CLAM and, when combined into an ensemble, often outperform state-of-the-art methods.
In future work, new ideas for additional algorithms that contribute novel inductive biases can be easily incorporated into CHAODA.

CLAM uses the geometric and topological properties such as fractal dimension of the data to build a map of the low-dimensional manifold that the data occupy.
CLAM is inspired by the clustering algorithm from CHESS~\cite{ishaq2019clustered} but differs by the introduction a novel graph-induction approach and a notion of optimal depths, learned via a form of ``meta-machine-learning'' and transfer learning.
While the standard approach in machine learning is to split a single dataset into many folds, CLAM and CHAODA are unusual in that they learn geometric and topological properties for inducing a graph from a \emph{set} of training datasets, and \emph{transfer} that knowledge to entirely different datasets.
CLAM's selection of ``poles'' (i.e. a pair of well-separated points) for partitioning clusters is also more efficient than CHESS.
Whereas CHESS was developed specifically for accelerating search,
CHAODA uses this manifold-mapping framework to discover properties of the manifold for anomaly and outlier detection.
Intuitively, we expect CHAODA to perform particularly well when the data lie on an ``interesting'' manifold, and to perform merely average when the data derive from an easily-described distribution (or ``boring'' manifold).
Just as CHESS demonstrated an acceleration of search when the data exhibited \emph{low fractal dimension} and \emph{low metric entropy}, with CHAODA, we see that AUC scores are vastly improved when the data exhibit these properties and often competitive when the data do not exhibit these properties.
We believe that CLAM is free of hyperparameters; the weights learned from the meta-ML step could vary, but we learned them once on a distinct training set.

We should note the Vertebral Column (Vert.) dataset.
Most tested algorithms perform similarly to random guessing, while CHAODA performs much worse.
We suspect this is due to how this specific dataset was collected.
Each instance represents six biomechanical attributes derived from scans of a patient's pelvis and lumbar spine.
This dataset contains 210 instances of the Abnormal class treated as inliers and 30 instances of the Normal class treated as outliers.
Each attribute has a narrow range to be in the Normal class but can have a wider range in the Abnormal class.
This causes the Normal instances to group together, while Abnormal instances remain distant from each other.
Since CHAODA relies on clusters as the substrate, it assigns low scores to instances in the Normal class, i.e.\ the outliers, and high scores to those in the Abnormal class, i.e.\ the inliers.
Put plainly, CHAODA sees the manifold as the Normal class, which the dataset labels as outliers.
This curiosity could be remedied by a supervised version of CHAODA.

The choice of distance function could significantly impact anomaly-detection performance.
In this case, domain knowledge is likely the best way to determine the distance function of choice.
Future work should explore a more diverse collection of domain-appropriate distance functions, such as Wasserstein distance on images, Levenshtein distance on strings, and Jaccard distance on the maximal common subgraph of molecular structures.
Currently, CLAM only functions on a Banach space defined by a distance metric.
This poses a limitation on datasets that have heterogenous features, such as a mix of continuous and categorical variables.
Future work should explore linear combinations of normalized distance functions to overcome this limitation.

% TODO new whole paragraph discussing suitability to large, HD datasets.

CHAODA is demonstrably highly effective on large high-dimensional datasets, and so may be applied to neural networks.
Using CLAM to map a dataset where each datum represents the activation-pattern of a neural network from an input to the neural network, we would expect to detect malicious inputs to neural networks based on the intuition that malicious inputs produce atypical activation patterns.

In conclusion, we have demonstrated that by mapping the manifolds occupied by data, CLAM reveals geometric and topological structure that allows CHAODA to outperform other state-of-the-art approaches to anomaly detection, representing an actualization of the manifold hypothesis.

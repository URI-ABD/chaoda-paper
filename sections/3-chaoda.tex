\section{CHAODA}
\label{sec:chaoda}

\subsection{CLAM}
\label{subsec:chaoda:manifold}

We present a Manifold-Mapping algorithm called CLAM (Clustered Learning of Approximate Manifolds).
This is an extension of earlier work presented in~\cite{ishaq2019entropy}.

To start, we need a dataset and a distance function on the points in that dataset.
A Dataset is a collection of $n$-points in a $D$-dimensional embedding space.
\begin{gather*}
    \textbf{X} = \{x_1 \dots x_n\}, x_i \in \mathbb{R}^D
\end{gather*}
A Distance Function takes two points in the dataset and deterministically produces a non-negative real number.
\begin{gather*}
    f : (\mathbb{R}^D, \mathbb{R}^D) \mapsto \mathbb{R}^+
\end{gather*}
We require any distance function to have the following properties:
\begin{align*}
    \forall x \in X, & \ f(x, x) = 0 \\
    \forall x, y \in X, & \ f(x, y) = f(y, x)
\end{align*}
The distance function may or may not obey the triangle-inequality.

\subsubsection*{Clustering:}
% describe creation of Cluster Tree
We start by building a divisive-hierarchical clustering of the data.
This gives us a tree of Clusters, with the root containing every point in the dataset, and each leaf containing a single point from the dataset.
The procedure is detailed in~\cite{ishaq2019entropy}.

Some important Cluster properties to consider are:
\begin{itemize}
    \item \textit{Cardinality}, i.e.\ the number of points in a cluster.
    \item \textit{Center}, i.e.\ the geometric median of points contained in a cluster.
    \item \textit{Radius}, i.e.\ the distance to the farthest point from a center.
    \item \textit{Local fractal dimension}, as described in~\cite{ishaq2019entropy}.
    \item \textit{Parent-Child ratios} of cardinality, radius, and local fractal dimension.
    \item \textit{Exponential moving averages} of the parent-child ratios along a branch of the tree.
\end{itemize}
In particular, we use the parent-child ratios and the exponential moving averages of those ratios to help generalize our anomaly detection method from a small set of datasets to a large, distinct set of datasets.

\subsubsection*{Graphs:}
% describe Graph and graph invariant.
Clusters that are near each other in the embedding space sometimes have overlapping volumes, i.e.\ the distance between their centers is less than or equal to the sum of their radii.
We define a Graph with the clusters as the nodes and with edges between overlapping clusters.
For our purposes, a Graph also has the following additional properties:
\begin{itemize}
    \item The clusters in the graph collectively contain every point in the dataset.
    \item Each point in the dataset is in exactly one cluster in the graph.
\end{itemize}
% Go over layer-graphs and optimal-graphs.
A Graph can be built from clusters at a fixed depth in the cluster-tree, or can contain clusters from multiple different depths in the tree.
We say that the cardinality of a graph is the number of clusters in that graph.

\subsubsection*{The Manifold:}
% describe Manifold as containing the tree and all the graphs.
According to the Manifold Hypothesis~\cite{fefferman2016testing}, datasets that come from constrained generating processes and are embedded in a high-dimensional space actually only occupy a low-dimensional manifold in that embedding space.

The graphs discussed so far map this low dimensional manifold in the original embedding space.
Different graph do this at different levels of local and/or global resolution.
Our aim is to properly build such a graph, i.e.\ we want to be ``properly zoomed-in'' to the various regions of the manifold formed by the data.
We can then apply various anomaly detection algorithms to these graphs.
These algorithms will often also incorporate information from the tree.

We describe some algorithms in~\ref{subsec:chaoda:individual-algorithms}.
While these algorithms are themselves fairly simple, the real challenge is in selecting the right clusters for the graphs that the algorithms operate on.
We will demonstrate CLAM to be such a powerful technique in Manifold-Mapping that even such simple algorithms as described in~\ref{subsec:chaoda:individual-algorithms} more often than not outperform state-ot-the-art anomaly detection algorithms.

% describe individual algorithms.
\subsection{Individual Algorithms}
\label{subsec:chaoda:individual-algorithms}

Here we describe several simple methods for anomaly detection.
Each of these methods uses a graph of clusters to calculate an anomalousness score for each point in the dataset.

\subsubsection{Relative Cluster Cardinality:}
We measure the anomalousness of a point by the cardinality of the cluster that point belongs to relative to the cardinalities of the other clusters in the graph.
Points in the same cluster are considered equally anomalous and points in clusters with relatively low cardinalities are considered more anomalous than points in clusters with relatively high cardinalities.

\subsubsection{Child-Parent Cardinality Ratio:}
As described in CHESS~\cite{ishaq2019entropy}, a cluster is partitioned by using its two maximally distant points as poles.
The points are split among children by whichever pole they are closer to.
Consider the fraction of points in a cluster that are assigned to each child.
If a child cluster only contains a small fraction of the points that its parent did, then we consider the points in that child cluster to be anomalous.
These child-parent cardinality ratios are accumulated for each point down its branch in the tree, terminating when the child cluster is a node in the graph.
Points with a low value of these accumulated ratios are considered more anomalous than points with a high value of these accumulated ratios.

\subsubsection{Graph Neighborhood Size:}
Given the graph with clusters and edges, consider the number of clusters reachable from a starting cluster within a given graph distance $k$.
We call this number the \textit{graph-neighborhood} of the starting cluster.
When $k$ is relatively small compared to the diameter of the graph, we can consider the relative \textit{graph-neighborhoods} of every cluster in the graph.
Points in clusters with small graph-neighborhoods are considered more anomalous than points in clusters with large graph-neighborhoods.

\subsubsection{Relative Subgraph Cardinality}
We define disconnected components of a graph by the property that no two clusters from different disconnected components have an edge between them.
Consider the relative cardinalities of each component in much the same we we considered the relative cardinalities of clusters in the Cluster Cardinality method.
Points in clusters in the same component are considered equally anomalous and points in clusters in relatively small components are considered more anomalous than points in clusters in relatively large components.


\subsection{Normalizing Anomaly Scores}
\label{subsec:chaoda:normalizing-anomaly-scores}

%Due to the wide range of possible ``anomalousness'' scores from our methods, as a final step, we normalize all of our measurements by scaling them.
%For this work, we utilized Min-Max Scaling, shown in Equation~\ref{sec:methods:min-max-normalizationn}, to ensure that anomalousness values always in the $[0, 1]$ range.
%This lets us compare anomalousness of points fairly across our various methods.
%
%\begin{gather}
%x^{\prime} = \frac{x - x_{min}}{x_{max} - x_{min}}
%\label{sec:chaoda:min-max-normalizationn}
%\end{gather}
TODO


\subsection{Meta Machine Learning}
\label{subsec:chaoda:meta-machine-learning}
% describe layer-graphs, and selected-graphs.
% describe meta-ml training-data creation from layer-graphs.
% justify meta-ml model choice.
TODO


\subsection{Selecting Better Graphs}
\label{subsec:chaoda:selecting-better-graphs}
% describe integration of meta-ml model into manifold and selection criteria for selected-graphs.
% describe ranking for selecting clusters for selected-graphs.
TODO


\subsection{Ensemble}
\label{subsec:chaoda:ensemble}
% describe ensemble-model built from all selected-graphs.
TODO

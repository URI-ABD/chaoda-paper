\documentclass[conference,compsoc]{IEEEtran}

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

% \usepackage{arxiv} # uncomment for preprint

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\score}{score}

% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath

\usepackage{placeins}
% \usepackage{algpseudocode}

% \algnewcommand\algorithmicforeach{\textbf{foreach}}
% \algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Clustered Hierarchical Anomaly and Outlier Detection Algorithms}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Najib Ishaq}
\IEEEauthorblockA{Computer Science and Statistics\\
University of Rhode Island\\
Kingston, RI 02881\\
najib\_ishaq@zoho.com}
\and
\IEEEauthorblockN{Thomas J. Howard III}
\IEEEauthorblockA{Computer Science and Statistics\\
University of Rhode Island\\
Kingston, RI 02881\\
thoward27@uri.edu}
\and
\IEEEauthorblockN{Noah M. Daniels}
\IEEEauthorblockA{Computer Science and Statistics\\
University of Rhode Island\\
Kingston, RI 02881\\
noah\_daniels@uri.edu}}

\IEEEoverridecommandlockouts
\IEEEpubid{\makebox[\columnwidth]{978-1-5386-5541-2/18/\$31.00~\copyright2018 IEEE \hfill} \hspace{\columnsep}\makebox[\columnwidth]{ }}
% make the title area
\maketitle
\IEEEpubidadjcol
% this must go after the closing bracket ] following \twocolumn[ ...

  \begin{abstract}
    Anomaly and outlier detection is a long-standing problem in machine learning.
    In some cases, anomaly detection is easy, such as when data are drawn from well-characterized distributions such as the Gaussian.
    However, when data occupy high-dimensional spaces, anomaly detection becomes more difficult.
    We present CLAM (Clustered Learning of Approximate Manifolds), a manifold mapping technique in a Banach space defined by a distance metric.
    CLAM begins with a fast hierarchical clustering technique and then induces a graph from the cluster tree, based on overlapping clusters as selected using several geometric and topological features.
    Using these graphs, we implement CHAODA (Clustered Hierarchical Anomaly and Outlier Detection Algorithms), exploring various properties of the graphs and their constituent clusters to find outliers.
    CHAODA employs a form of transfer learning based on a training set of datasets, and applies this knowledge to a separate test set of datasets of different cardinalities, dimensionalities, and domains.
    On 24 publicly available datasets, we compare CHAODA (by measure of ROC AUC) to a variety of state-of-the-art unsupervised anomaly-detection algorithms.
    Six of the datasets are used for training.
    CHAODA outperforms other approaches on 16 of the remaining 18 datasets.
    CLAM and CHAODA scale to large, high-dimensional ``big data'' anomaly-detection problems, and generalize across datasets and distance functions.
    Source code to CLAM and CHAODA are freely available on GitHub\footnote{https://github.com/URI-ABD/clam}.
  \end{abstract}

\IEEEpeerreviewmaketitle

    \input{sections/1-introduction.tex}
    \input{sections/2-methods.tex}
    \input{sections/3-results.tex}
    \input{sections/4-discussion.tex}
    %  Acknowledgments are not allowed in the blind review version
    % (damn! they're onto us!)
    % \input{sections/5-acknowledgements.tex}

    % \afterpage{\clearpage}
    \FloatBarrier
    \bibliographystyle{IEEEtran}
    \bibliography{references}
\end{document}
